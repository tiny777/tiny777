

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://resource.tinychen.com/logos.png">
  <link rel="icon" href="https://resource.tinychen.com/logos.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="TinyChen">
  <meta name="keywords" content="">
  
    <meta name="description" content="本文主要在centos7系统上基于containerd和stable版本（1.12.4）的cilium组件部署v1.25.4版本的堆叠ETCD高可用k8s原生集群，在LoadBalancer上选择了PureLB和kube-router结合cilium实现BGP路由可达的K8S集群部署。 此前写的一些关于k8s基础知识和集群搭建的一些方案，有需要的同学可以看一下。">
<meta property="og:type" content="article">
<meta property="og:title" content="k8s系列10-使用kube-router和cilium部署BGP模式的k8s集群">
<meta property="og:url" content="https://tinychen.com/20221209-k8s-10-deploy-ha-k8s-with-cilium-bgp/index.html">
<meta property="og:site_name" content="TinyChen&#39;s Studio - 互联网技术学习工作经验分享">
<meta property="og:description" content="本文主要在centos7系统上基于containerd和stable版本（1.12.4）的cilium组件部署v1.25.4版本的堆叠ETCD高可用k8s原生集群，在LoadBalancer上选择了PureLB和kube-router结合cilium实现BGP路由可达的K8S集群部署。 此前写的一些关于k8s基础知识和集群搭建的一些方案，有需要的同学可以看一下。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://resource.tinychen.com/202212092303054.jpg">
<meta property="article:published_time" content="2022-12-09T15:00:00.000Z">
<meta property="article:modified_time" content="2022-12-09T15:00:00.000Z">
<meta property="article:author" content="TinyChen">
<meta property="article:tag" content="centos">
<meta property="article:tag" content="k8s">
<meta property="article:tag" content="docker">
<meta property="article:tag" content="cilium">
<meta property="article:tag" content="purelb">
<meta property="article:tag" content="bgp">
<meta property="article:tag" content="containerd">
<meta property="article:tag" content="ebpf">
<meta property="article:tag" content="xdp">
<meta property="article:tag" content="kube-router">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://resource.tinychen.com/202212092303054.jpg">
  
  
  <title>k8s系列10-使用kube-router和cilium部署BGP模式的k8s集群 - TinyChen&#39;s Studio - 互联网技术学习工作经验分享</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/dracula.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/fluid-extention.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"tinychen.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":30,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"7a96963a1145ac7fde1442d739a11ffd","google":"UA-166769908-1","gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="TinyChen's Studio - 互联网技术学习工作经验分享" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>TinyChen</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://resource.tinychen.com/202212092304776.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="k8s系列10-使用kube-router和cilium部署BGP模式的k8s集群">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-12-09 23:00" pubdate>
        December 9, 2022 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      69k 字
    </span>
  

  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">k8s系列10-使用kube-router和cilium部署BGP模式的k8s集群</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：December 9, 2022 pm
                
              </p>
            
            <div class="markdown-body">
              <p>本文主要在centos7系统上基于<code>containerd</code>和<code>stable</code>版本（<code>1.12.4</code>）的<code>cilium</code>组件部署<code>v1.25.4</code>版本的堆叠ETCD高可用k8s原生集群，在<code>LoadBalancer</code>上选择了<code>PureLB</code>和<code>kube-router</code>结合<code>cilium</code>实现<strong>BGP路由可达</strong>的K8S集群部署。</p>
<p>此前写的一些关于k8s基础知识和集群搭建的一些<a href="https://tinychen.com/tags/k8s/">方案</a>，有需要的同学可以看一下。</p>
<span id="more"></span>
<h1>1、准备工作</h1>
<h2 id="1-1-集群信息">1.1 集群信息</h2>
<p>机器均为16C16G的虚拟机，硬盘为100G。</p>
<table>
<thead>
<tr>
<th style="text-align:center">IP</th>
<th style="text-align:center">Hostname</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">10.31.80.0</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="http://k8s-cilium-apiserver.tinychen.io">k8s-cilium-apiserver.tinychen.io</a></td>
</tr>
<tr>
<td style="text-align:center">10.31.80.1</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="http://k8s-cilium-master-10-31-80-1.tinychen.io">k8s-cilium-master-10-31-80-1.tinychen.io</a></td>
</tr>
<tr>
<td style="text-align:center">10.31.80.2</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="http://k8s-cilium-master-10-31-80-2.tinychen.io">k8s-cilium-master-10-31-80-2.tinychen.io</a></td>
</tr>
<tr>
<td style="text-align:center">10.31.80.3</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="http://k8s-cilium-master-10-31-80-3.tinychen.io">k8s-cilium-master-10-31-80-3.tinychen.io</a></td>
</tr>
<tr>
<td style="text-align:center">10.31.80.4</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="http://k8s-cilium-worker-10-31-80-4.tinychen.io">k8s-cilium-worker-10-31-80-4.tinychen.io</a></td>
</tr>
<tr>
<td style="text-align:center">10.31.80.5</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="http://k8s-cilium-worker-10-31-80-5.tinychen.io">k8s-cilium-worker-10-31-80-5.tinychen.io</a></td>
</tr>
<tr>
<td style="text-align:center">10.31.80.6</td>
<td style="text-align:center"><a target="_blank" rel="noopener" href="http://k8s-cilium-worker-10-31-80-6.tinychen.io">k8s-cilium-worker-10-31-80-6.tinychen.io</a></td>
</tr>
<tr>
<td style="text-align:center">10.32.0.0/17</td>
<td style="text-align:center">podSubnet</td>
</tr>
<tr>
<td style="text-align:center">10.32.128.0/18</td>
<td style="text-align:center">serviceSubnet</td>
</tr>
<tr>
<td style="text-align:center">10.32.192.0/18</td>
<td style="text-align:center">LoadBalancerSubnet</td>
</tr>
</tbody>
</table>
<h2 id="1-2-检查mac和product-uuid">1.2 检查mac和product_uuid</h2>
<p>同一个k8s集群内的所有节点需要确保<code>mac</code>地址和<code>product_uuid</code>均唯一，开始集群初始化之前需要检查相关信息</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 检查mac地址</span><br>ip <span class="hljs-built_in">link</span> <br>ifconfig -a<br><br><span class="hljs-comment"># 检查product_uuid</span><br>sudo <span class="hljs-built_in">cat</span> /sys/class/dmi/id/product_uuid<br></code></pre></div></td></tr></table></figure>
<h2 id="1-3-配置ssh免密登录（可选）">1.3 配置ssh免密登录（可选）</h2>
<p>如果k8s集群的节点有多个网卡，确保每个节点能通过正确的网卡互联访问</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 在root用户下面生成一个公用的key，并配置可以使用该key免密登录</span><br>su root<br>ssh-keygen<br><span class="hljs-built_in">cd</span> /root/.ssh/<br><span class="hljs-built_in">cat</span> id_rsa.pub &gt;&gt; authorized_keys<br><span class="hljs-built_in">chmod</span> 600 authorized_keys<br><br><br><span class="hljs-built_in">cat</span> &gt;&gt; ~/.ssh/config &lt;&lt;<span class="hljs-string">EOF</span><br><span class="hljs-string">Host k8s-cilium-master-10-31-80-1</span><br><span class="hljs-string">    HostName 10.31.80.1</span><br><span class="hljs-string">    User root</span><br><span class="hljs-string">    Port 22</span><br><span class="hljs-string">    IdentityFile ~/.ssh/id_rsa</span><br><span class="hljs-string"></span><br><span class="hljs-string">Host k8s-cilium-master-10-31-80-2</span><br><span class="hljs-string">    HostName 10.31.80.2</span><br><span class="hljs-string">    User root</span><br><span class="hljs-string">    Port 22</span><br><span class="hljs-string">    IdentityFile ~/.ssh/id_rsa</span><br><span class="hljs-string"></span><br><span class="hljs-string">Host k8s-cilium-master-10-31-80-3</span><br><span class="hljs-string">    HostName 10.31.80.3</span><br><span class="hljs-string">    User root</span><br><span class="hljs-string">    Port 22</span><br><span class="hljs-string">    IdentityFile ~/.ssh/id_rsa</span><br><span class="hljs-string"></span><br><span class="hljs-string">Host k8s-cilium-worker-10-31-80-4</span><br><span class="hljs-string">    HostName 10.31.80.4</span><br><span class="hljs-string">    User root</span><br><span class="hljs-string">    Port 22</span><br><span class="hljs-string">    IdentityFile ~/.ssh/id_rsa</span><br><span class="hljs-string"></span><br><span class="hljs-string">Host k8s-cilium-worker-10-31-80-5</span><br><span class="hljs-string">    HostName 10.31.80.5</span><br><span class="hljs-string">    User root</span><br><span class="hljs-string">    Port 22</span><br><span class="hljs-string">    IdentityFile ~/.ssh/id_rsa</span><br><span class="hljs-string"></span><br><span class="hljs-string">Host k8s-cilium-worker-10-31-80-6</span><br><span class="hljs-string">    HostName 10.31.80.6</span><br><span class="hljs-string">    User root</span><br><span class="hljs-string">    Port 22</span><br><span class="hljs-string">    IdentityFile ~/.ssh/id_rsa</span><br><span class="hljs-string">EOF</span><br></code></pre></div></td></tr></table></figure>
<h2 id="1-4-修改hosts文件">1.4 修改hosts文件</h2>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> &gt;&gt; /etc/hosts &lt;&lt;<span class="hljs-string">EOF</span><br><span class="hljs-string">10.31.80.1 k8s-cilium-master-10-31-80-1 k8s-cilium-master-10-31-80-1.tinychen.io</span><br><span class="hljs-string">10.31.80.2 k8s-cilium-master-10-31-80-2 k8s-cilium-master-10-31-80-2.tinychen.io</span><br><span class="hljs-string">10.31.80.3 k8s-cilium-master-10-31-80-3 k8s-cilium-master-10-31-80-3.tinychen.io</span><br><span class="hljs-string">10.31.80.4 k8s-cilium-worker-10-31-80-4 k8s-cilium-worker-10-31-80-4.tinychen.io</span><br><span class="hljs-string">10.31.80.5 k8s-cilium-worker-10-31-80-5 k8s-cilium-worker-10-31-80-5.tinychen.io</span><br><span class="hljs-string">10.31.80.6 k8s-cilium-worker-10-31-80-6 k8s-cilium-worker-10-31-80-6.tinychen.io</span><br><span class="hljs-string">10.31.80.0 k8s-cilium-apiserver k8s-cilium-apiserver.tinychen.io</span><br><span class="hljs-string">EOF</span><br></code></pre></div></td></tr></table></figure>
<h2 id="1-5-关闭swap内存">1.5 关闭swap内存</h2>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 使用命令直接关闭swap内存</span><br>swapoff -a<br><span class="hljs-comment"># 修改fstab文件禁止开机自动挂载swap分区</span><br>sed -i <span class="hljs-string">&#x27;/swap / s/^\(.*\)$/#\1/g&#x27;</span> /etc/fstab<br></code></pre></div></td></tr></table></figure>
<h2 id="1-6-配置时间同步">1.6 配置时间同步</h2>
<p>这里可以根据自己的习惯选择ntp或者是chrony同步均可，同步的时间源服务器可以选择阿里云的<code>ntp1.aliyun.com</code>或者是国家时间中心的<code>ntp.ntsc.ac.cn</code>。</p>
<h3 id="使用ntp同步">使用ntp同步</h3>
<figure class="highlight cmake"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs cmake"><span class="hljs-comment"># 使用yum安装ntpdate工具</span><br>yum <span class="hljs-keyword">install</span> ntpdate -y<br><br><span class="hljs-comment"># 使用国家时间中心的源同步时间</span><br>ntpdate ntp.ntsc.ac.cn<br><br><span class="hljs-comment"># 最后查看一下时间</span><br>hwclock<br></code></pre></div></td></tr></table></figure>
<h3 id="使用chrony同步">使用chrony同步</h3>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 使用yum安装chrony</span><br>yum install chrony -y<br><br><span class="hljs-comment"># 设置开机启动并开启chony并查看运行状态</span><br>systemctl <span class="hljs-built_in">enable</span> chronyd.service<br>systemctl start chronyd.service<br>systemctl status chronyd.service<br><br><span class="hljs-comment"># 当然也可以自定义时间服务器</span><br>vim /etc/chrony.conf<br><br><span class="hljs-comment"># 修改前</span><br>$ grep server /etc/chrony.conf<br><span class="hljs-comment"># Use public servers from the pool.ntp.org project.</span><br>server 0.centos.pool.ntp.org iburst<br>server 1.centos.pool.ntp.org iburst<br>server 2.centos.pool.ntp.org iburst<br>server 3.centos.pool.ntp.org iburst<br><br><span class="hljs-comment"># 修改后</span><br>$ grep server /etc/chrony.conf<br><span class="hljs-comment"># Use public servers from the pool.ntp.org project.</span><br>server ntp.ntsc.ac.cn iburst<br><br><span class="hljs-comment"># 重启服务使配置文件生效</span><br>systemctl restart chronyd.service<br><br><span class="hljs-comment"># 查看chrony的ntp服务器状态</span><br>chronyc sourcestats -v<br>chronyc sources -v<br><br></code></pre></div></td></tr></table></figure>
<h2 id="1-7-关闭selinux">1.7 关闭selinux</h2>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 使用命令直接关闭</span><br>setenforce 0<br><br><span class="hljs-comment"># 也可以直接修改/etc/selinux/config文件</span><br>sed -i <span class="hljs-string">&#x27;s/^SELINUX=enforcing$/SELINUX=disabled/&#x27;</span> /etc/selinux/config<br></code></pre></div></td></tr></table></figure>
<h2 id="1-8-配置防火墙">1.8 配置防火墙</h2>
<p>k8s集群之间通信和服务暴露需要使用较多端口，为了方便，直接禁用防火墙</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># centos7使用systemctl禁用默认的firewalld服务</span><br>systemctl <span class="hljs-built_in">disable</span> firewalld.service<br></code></pre></div></td></tr></table></figure>
<h2 id="1-9-配置netfilter参数">1.9 配置netfilter参数</h2>
<p>这里主要是需要配置内核加载<code>br_netfilter</code>和<code>iptables</code>放行<code>ipv6</code>和<code>ipv4</code>的流量，确保集群内的容器能够正常通信。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF | sudo tee /etc/modules-load.d/k8s.conf</span><br><span class="hljs-string">br_netfilter</span><br><span class="hljs-string">EOF</span><br><br><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF | sudo tee /etc/sysctl.d/k8s.conf</span><br><span class="hljs-string">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="hljs-string">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="hljs-string">EOF</span><br>sudo sysctl --system<br></code></pre></div></td></tr></table></figure>
<h2 id="1-10-关闭IPV6（不建议）">1.10 关闭IPV6（不建议）</h2>
<p>和之前部署其他的CNI不一样，cilium很多服务监听默认情况下都是双栈的（使用cilium-cli操作的时候），因此建议开启系统的IPV6网络支持（即使没有可用的IPV6路由也可以）</p>
<p>当然没有ipv6网络也是可以的，只是在使用cilium-cli的一些开启port-forward命令时会报错而已。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 直接在内核中添加ipv6禁用参数</span><br>grubby --update-kernel=ALL --args=ipv6.disable=1<br></code></pre></div></td></tr></table></figure>
<h2 id="1-11-配置IPVS">1.11 配置IPVS</h2>
<p>IPVS是专门设计用来应对负载均衡场景的组件，<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md#run-kube-proxy-in-ipvs-mode">kube-proxy 中的 IPVS 实现</a>通过减少对 iptables 的使用来增加可扩展性。在 iptables 输入链中不使用 PREROUTING，而是创建一个假的接口，叫做 kube-ipvs0，当k8s集群中的负载均衡配置变多的时候，IPVS能实现比iptables更高效的转发性能。</p>
<blockquote>
<p>因为cilium需要升级系统内核，因此这里的内核版本高于4.19</p>
<p>注意在4.19之后的内核版本中使用<code>nf_conntrack</code>模块来替换了原有的<code>nf_conntrack_ipv4</code>模块</p>
<p>(<strong>Notes</strong>: use <code>nf_conntrack</code> instead of <code>nf_conntrack_ipv4</code> for Linux kernel 4.19 and later)</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 在使用ipvs模式之前确保安装了ipset和ipvsadm</span><br>sudo yum install ipset ipvsadm -y<br><br><span class="hljs-comment"># 手动加载ipvs相关模块</span><br>modprobe -- ip_vs<br>modprobe -- ip_vs_rr<br>modprobe -- ip_vs_wrr<br>modprobe -- ip_vs_sh<br>modprobe -- nf_conntrack<br><br><span class="hljs-comment"># 配置开机自动加载ipvs相关模块</span><br><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF | sudo tee /etc/modules-load.d/ipvs.conf</span><br><span class="hljs-string">ip_vs</span><br><span class="hljs-string">ip_vs_rr</span><br><span class="hljs-string">ip_vs_wrr</span><br><span class="hljs-string">ip_vs_sh</span><br><span class="hljs-string">nf_conntrack</span><br><span class="hljs-string">EOF</span><br><br><br>$ lsmod | grep -e ip_vs -e nf_conntrack<br>nf_conntrack_netlink    49152  0<br>nfnetlink              20480  2 nf_conntrack_netlink<br>ip_vs_sh               16384  0<br>ip_vs_wrr              16384  0<br>ip_vs_rr               16384  0<br>ip_vs                 159744  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr<br>nf_conntrack          159744  5 xt_conntrack,nf_nat,nf_conntrack_netlink,xt_MASQUERADE,ip_vs<br>nf_defrag_ipv4         16384  1 nf_conntrack<br>nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs<br>libcrc32c              16384  4 nf_conntrack,nf_nat,xfs,ip_vs<br>$ <span class="hljs-built_in">cut</span> -f1 -d <span class="hljs-string">&quot; &quot;</span>  /proc/modules | grep -e ip_vs -e nf_conntrack<br>nf_conntrack_netlink<br>ip_vs_sh<br>ip_vs_wrr<br>ip_vs_rr<br>ip_vs<br>nf_conntrack<br></code></pre></div></td></tr></table></figure>
<h2 id="1-12-配置Linux内核（cilium必选）">1.12 配置Linux内核（cilium必选）</h2>
<p>cilium和其他的cni组件最大的不同在于其底层使用了ebpf技术，而该技术对于Linux的系统内核版本有较高的要求，完成的要求可以查看官网的<a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/operations/system_requirements/">详细链接</a>，这里我们着重看内核版本、内核参数这两个部分。</p>
<h3 id="Linux内核版本">Linux内核版本</h3>
<p>默认情况下我们可以参考cilium官方给出的一个系统要求总结。因为我们是在k8s集群中部署（使用容器），因此只需要关注Linux内核版本和etcd版本即可。根据前面部署的经验我们可以知道1.23.6版本的k8s默认使用的etcd版本是<code>3.5.+</code>，因此重点就来到了Linux内核版本这里。</p>
<table>
<thead>
<tr>
<th>Requirement</th>
<th>Minimum Version</th>
<th>In cilium container</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/operations/system_requirements/#linux-kernel">Linux kernel</a></td>
<td>&gt;= 4.9.17</td>
<td>no</td>
</tr>
<tr>
<td>Key-Value store (etcd)</td>
<td>&gt;= 3.1.0</td>
<td>no</td>
</tr>
<tr>
<td>clang+LLVM</td>
<td>&gt;= 10.0</td>
<td>yes</td>
</tr>
<tr>
<td>iproute2</td>
<td>&gt;= 5.9.0</td>
<td>yes</td>
</tr>
</tbody>
</table>
<blockquote>
<p>This requirement is only needed if you run <code>cilium-agent</code> natively. If you are using the Cilium container image <code>cilium/cilium</code>, clang+LLVM is included in the container image.</p>
<p>iproute2 is only needed if you run <code>cilium-agent</code> directly on the host machine. iproute2 is included in the <code>cilium/cilium</code> container image.</p>
</blockquote>
<p>毫无疑问CentOS7内置的默认内核版本3.10.x版本的内核是无法满足需求的，但是在升级内核之前，我们再看看其他的一些要求。</p>
<p>cilium官方还给出了<a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/operations/system_requirements/#required-kernel-versions-for-advanced-features">一份列表</a>描述了各项高级功能对内核版本的要求：</p>
<table>
<thead>
<tr>
<th>Cilium Feature</th>
<th>Minimum Kernel Version</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/concepts/networking/fragmentation/#concepts-fragmentation">IPv4 fragment handling</a></td>
<td>&gt;= 4.10</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/operations/upgrade/#cidr-limitations">Restrictions on unique prefix lengths for CIDR policy rules</a></td>
<td>&gt;= 4.11</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/gettingstarted/encryption-ipsec/#encryption-ipsec">IPsec Transparent Encryption</a> in tunneling mode</td>
<td>&gt;= 4.19</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/gettingstarted/encryption-wireguard/#encryption-wg">WireGuard Transparent Encryption</a></td>
<td>&gt;= 5.6</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/gettingstarted/host-services/#host-services">Host-Reachable Services</a></td>
<td>&gt;= 4.19.57, &gt;= 5.1.16, &gt;= 5.2</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/gettingstarted/kubeproxy-free/#kubeproxy-free">Kubernetes Without kube-proxy</a></td>
<td>&gt;= 4.19.57, &gt;= 5.1.16, &gt;= 5.2</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/gettingstarted/bandwidth-manager/#bandwidth-manager">Bandwidth Manager</a></td>
<td>&gt;= 5.1</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/gettingstarted/local-redirect-policy/#local-redirect-policy">Local Redirect Policy (beta)</a></td>
<td>&gt;= 4.19.57, &gt;= 5.1.16, &gt;= 5.2</td>
</tr>
<tr>
<td>Full support for <a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/gettingstarted/kubeproxy-free/#session-affinity">Session Affinity</a></td>
<td>&gt;= 5.7</td>
</tr>
<tr>
<td>BPF-based proxy redirection</td>
<td>&gt;= 5.7</td>
</tr>
<tr>
<td>BPF-based host routing</td>
<td>&gt;= 5.10</td>
</tr>
<tr>
<td>Socket-level LB bypass in pod netns</td>
<td>&gt;= 5.7</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/gettingstarted/egress-gateway/#egress-gateway">Egress Gateway (beta)</a></td>
<td>&gt;= 5.2</td>
</tr>
<tr>
<td>VXLAN Tunnel Endpoint (VTEP) Integration</td>
<td>&gt;= 5.2</td>
</tr>
</tbody>
</table>
<p>可以看到如果需要满足上面所有需求的话，需要内核版本高于5.10，本着学习测试研究作死的精神，反正都升级了，干脆就升级到新一些的版本吧。这里我们可以直接<a href="https://tinychen.com/20190612-centos-update-kernel/">使用elrepo源来升级内核</a>到较新的内核版本。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 查看elrepo源中支持的内核版本</span><br>$ yum --disablerepo=<span class="hljs-string">&quot;*&quot;</span> --enablerepo=<span class="hljs-string">&quot;elrepo-kernel&quot;</span> list available<br>Loaded plugins: fastestmirror<br>Loading mirror speeds from cached hostfile<br>Available Packages<br>elrepo-release.noarch                                   7.0-6.el7.elrepo                                            elrepo-kernel<br>kernel-lt.x86_64                                        5.4.225-1.el7.elrepo                                        elrepo-kernel<br>kernel-lt-devel.x86_64                                  5.4.225-1.el7.elrepo                                        elrepo-kernel<br>kernel-lt-doc.noarch                                    5.4.225-1.el7.elrepo                                        elrepo-kernel<br>kernel-lt-headers.x86_64                                5.4.225-1.el7.elrepo                                        elrepo-kernel<br>kernel-lt-tools.x86_64                                  5.4.225-1.el7.elrepo                                        elrepo-kernel<br>kernel-lt-tools-libs.x86_64                             5.4.225-1.el7.elrepo                                        elrepo-kernel<br>kernel-lt-tools-libs-devel.x86_64                       5.4.225-1.el7.elrepo                                        elrepo-kernel<br>kernel-ml.x86_64                                        6.0.11-1.el7.elrepo                                         elrepo-kernel<br>kernel-ml-devel.x86_64                                  6.0.11-1.el7.elrepo                                         elrepo-kernel<br>kernel-ml-doc.noarch                                    6.0.11-1.el7.elrepo                                         elrepo-kernel<br>kernel-ml-headers.x86_64                                6.0.11-1.el7.elrepo                                         elrepo-kernel<br>kernel-ml-tools.x86_64                                  6.0.11-1.el7.elrepo                                         elrepo-kernel<br>kernel-ml-tools-libs.x86_64                             6.0.11-1.el7.elrepo                                         elrepo-kernel<br>kernel-ml-tools-libs-devel.x86_64                       6.0.11-1.el7.elrepo                                         elrepo-kernel<br>perf.x86_64                                             5.4.225-1.el7.elrepo                                        elrepo-kernel<br>python-perf.x86_64                                      5.4.225-1.el7.elrepo                                        elrepo-kernel<br><br><span class="hljs-comment"># 看起来ml版本的内核比较满足我们的需求,直接使用yum进行安装</span><br>sudo yum --enablerepo=elrepo-kernel install kernel-ml -y<br><span class="hljs-comment"># 使用grubby工具查看系统中已经安装的内核版本信息</span><br>sudo grubby --info=ALL<br><span class="hljs-comment"># 设置新安装的6.0.11-1版本内核为默认内核版本，此处的index=0要和上面查看的内核版本信息一致</span><br>sudo grubby --set-default-index=0<br><span class="hljs-comment"># 查看默认内核是否修改成功</span><br>sudo grubby --default-kernel<br><span class="hljs-comment"># 重启系统切换到新内核</span><br>init 6<br><span class="hljs-comment"># 重启后检查内核版本是否为新的6.0.11-1</span><br><span class="hljs-built_in">uname</span> -a<br></code></pre></div></td></tr></table></figure>
<h3 id="Linux内核参数">Linux内核参数</h3>
<p>首先我们查看自己当前内核版本的参数，基本上可以分为<code>y</code>、<code>n</code>、<code>m</code>三个选项</p>
<ul>
<li>y：yes，Build directly into the kernel. 表示该功能被编译进内核中，默认启用</li>
<li>n：no，Leave entirely out of the kernel. 表示该功能未被编译进内核中，不启用</li>
<li>m：module，Build as a module, to be loaded if needed. 表示该功能被编译为模块，按需启用</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 查看当前使用的内核版本的编译参数</span><br><span class="hljs-built_in">cat</span> /boot/config-$(<span class="hljs-built_in">uname</span> -r)<br></code></pre></div></td></tr></table></figure>
<p>cilium官方对各项功能所需要开启的<a target="_blank" rel="noopener" href="https://docs.cilium.io/en/latest/operations/system_requirements/#linux-kernel">内核参数列举</a>如下：</p>
<blockquote>
<p>In order for the eBPF feature to be enabled properly, the following kernel configuration options must be enabled. This is typically the case with distribution kernels. When an option can be built as a module or statically linked, either choice is valid.</p>
<p>为了正确启用 eBPF 功能，必须启用以下内核配置选项。这通常因内核版本情况而异。任何一个选项都可以构建为模块或静态链接，两个选择都是有效的。</p>
</blockquote>
<p>我们暂时只看最基本的<code>Base Requirements</code></p>
<figure class="highlight ini"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs ini"><span class="hljs-attr">CONFIG_BPF</span>=y<br><span class="hljs-attr">CONFIG_BPF_SYSCALL</span>=y<br><span class="hljs-attr">CONFIG_NET_CLS_BPF</span>=y<br><span class="hljs-attr">CONFIG_BPF_JIT</span>=y<br><span class="hljs-attr">CONFIG_NET_CLS_ACT</span>=y<br><span class="hljs-attr">CONFIG_NET_SCH_INGRESS</span>=y<br><span class="hljs-attr">CONFIG_CRYPTO_SHA1</span>=y<br><span class="hljs-attr">CONFIG_CRYPTO_USER_API_HASH</span>=y<br><span class="hljs-attr">CONFIG_CGROUPS</span>=y<br><span class="hljs-attr">CONFIG_CGROUP_BPF</span>=y<br></code></pre></div></td></tr></table></figure>
<p>对比我们使用的<code>6.0.11-1.el7.elrepo.x86_64</code>内核可以发现有两个模块是为m</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># egrep &quot;^CONFIG_BPF=|^CONFIG_BPF_SYSCALL=|^CONFIG_NET_CLS_BPF=|^CONFIG_BPF_JIT=|^CONFIG_NET_CLS_ACT=|^CONFIG_NET_SCH_INGRESS=|^CONFIG_CRYPTO_SHA1=|^CONFIG_CRYPTO_USER_API_HASH=|^CONFIG_CGROUPS=|^CONFIG_CGROUP_BPF=&quot; /boot/config-6.0.11-1.el7.elrepo.x86_64</span><br>CONFIG_BPF=y<br>CONFIG_BPF_SYSCALL=y<br>CONFIG_BPF_JIT=y<br>CONFIG_CGROUPS=y<br>CONFIG_CGROUP_BPF=y<br>CONFIG_NET_SCH_INGRESS=m<br>CONFIG_NET_CLS_BPF=m<br>CONFIG_NET_CLS_ACT=y<br>CONFIG_CRYPTO_SHA1=y<br>CONFIG_CRYPTO_USER_API_HASH=y<br></code></pre></div></td></tr></table></figure>
<p>缺少的这两个模块我们可以在<code>/usr/lib/modules/$(uname -r)</code>目录下面找到它们：</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ <span class="hljs-built_in">realpath</span> ./kernel/net/sched/sch_ingress.ko<br>/usr/lib/modules/6.0.11-1.el7.elrepo.x86_64/kernel/net/sched/sch_ingress.ko<br>$ <span class="hljs-built_in">realpath</span> ./kernel/net/sched/cls_bpf.ko<br>/usr/lib/modules/6.0.11-1.el7.elrepo.x86_64/kernel/net/sched/cls_bpf.ko<br></code></pre></div></td></tr></table></figure>
<p>确认相关内核模块存在我们直接加载内核即可：</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 直接使用modprobe命令加载</span><br>$ modprobe cls_bpf<br>$ modprobe sch_ingress<br>$ lsmod | egrep <span class="hljs-string">&quot;cls_bpf|sch_ingress&quot;</span><br>sch_ingress            16384  0<br>cls_bpf                24576  0<br><br><span class="hljs-comment"># 配置开机自动加载cilium所需相关模块</span><br><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF | sudo tee /etc/modules-load.d/cilium-base-requirements.conf</span><br><span class="hljs-string">cls_bpf</span><br><span class="hljs-string">sch_ingress</span><br><span class="hljs-string">EOF</span><br></code></pre></div></td></tr></table></figure>
<p>其他cilium高级功能所需要的内核功能也类似，这里不做赘述。</p>
<h1>2、安装container runtime</h1>
<h2 id="2-1-安装containerd">2.1 安装containerd</h2>
<p>详细的官方文档可以参考<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">这里</a>，由于在刚发布的1.24版本中移除了<code>docker-shim</code>，因此安装的<code>版本≥1.24</code>的时候需要注意<code>容器运行时</code>的选择。这里我们安装的版本为高于1.24，因此我们不能继续使用docker，这里我们将其换为<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd">containerd</a></p>
<h3 id="修改Linux内核参数">修改Linux内核参数</h3>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 首先生成配置文件确保配置持久化</span><br><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF | sudo tee /etc/modules-load.d/containerd.conf</span><br><span class="hljs-string">overlay</span><br><span class="hljs-string">br_netfilter</span><br><span class="hljs-string">EOF</span><br><br>sudo modprobe overlay<br>sudo modprobe br_netfilter<br><br><span class="hljs-comment"># Setup required sysctl params, these persist across reboots.</span><br><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf</span><br><span class="hljs-string">net.bridge.bridge-nf-call-iptables  = 1</span><br><span class="hljs-string">net.ipv4.ip_forward                 = 1</span><br><span class="hljs-string">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="hljs-string">EOF</span><br><br><span class="hljs-comment"># Apply sysctl params without reboot</span><br>sudo sysctl --system<br></code></pre></div></td></tr></table></figure>
<h3 id="安装containerd">安装containerd</h3>
<p>centos7比较方便的部署方式是利用已有的yum源进行安装，这里我们可以使用docker官方的yum源来安装<code>containerd</code></p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 导入docker官方的yum源</span><br>sudo yum install -y yum-utils device-mapper-persistent-data lvm2<br><br>sudo yum-config-manager --add-repo  https://download.docker.com/linux/centos/docker-ce.repo<br><br><span class="hljs-comment"># 查看yum源中存在的各个版本的containerd.io</span><br>yum list containerd.io --showduplicates | <span class="hljs-built_in">sort</span> -r<br><br><span class="hljs-comment"># 直接安装最新版本的containerd.io</span><br>yum install containerd.io -y<br><br><span class="hljs-comment"># 启动containerd</span><br>sudo systemctl start containerd<br><br><span class="hljs-comment"># 最后我们还要设置一下开机启动</span><br>sudo systemctl <span class="hljs-built_in">enable</span> --now containerd<br><br></code></pre></div></td></tr></table></figure>
<h3 id="关于CRI">关于CRI</h3>
<p>官方表示，对于k8s来说，不需要安装<code>cri-containerd</code>，并且该功能会在后面的2.0版本中废弃。</p>
<blockquote>
<p><strong>FAQ</strong>: For Kubernetes, do I need to download <code>cri-containerd-(cni-)&lt;VERSION&gt;-&lt;OS-&lt;ARCH&gt;.tar.gz</code> too?</p>
<p><strong>Answer</strong>: No.</p>
<p>As the Kubernetes CRI feature has been already included in <code>containerd-&lt;VERSION&gt;-&lt;OS&gt;-&lt;ARCH&gt;.tar.gz</code>, you do not need to download the <code>cri-containerd-....</code> archives to use CRI.</p>
<p>The <code>cri-containerd-...</code> archives are <a target="_blank" rel="noopener" href="https://github.com/containerd/containerd/blob/main/RELEASES.md#deprecated-features">deprecated</a>, do not work on old Linux distributions, and will be removed in containerd 2.0.</p>
</blockquote>
<h3 id="安装cni-plugins">安装cni-plugins</h3>
<p>使用yum源安装的方式会把runc安装好，但是并不会安装cni-plugins，因此这部分还是需要我们自行安装。</p>
<blockquote>
<p>The <code>containerd.io</code> package contains runc too, but does not contain CNI plugins.</p>
</blockquote>
<p>我们直接在<a target="_blank" rel="noopener" href="https://github.com/containernetworking/plugins/releases">github上面</a>找到系统对应的架构版本，这里为amd64，然后解压即可。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># Download the cni-plugins-&lt;OS&gt;-&lt;ARCH&gt;-&lt;VERSION&gt;.tgz archive from https://github.com/containernetworking/plugins/releases , verify its sha256sum, and extract it under /opt/cni/bin:</span><br><br><span class="hljs-comment"># 下载源文件和sha512文件并校验</span><br>$ wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz<br>$ wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz.sha512<br>$ <span class="hljs-built_in">sha512sum</span> -c cni-plugins-linux-amd64-v1.1.1.tgz.sha512<br><br><span class="hljs-comment"># 创建目录并解压</span><br>$ <span class="hljs-built_in">mkdir</span> -p /opt/cni/bin<br>$ tar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.1.1.tgz<br></code></pre></div></td></tr></table></figure>
<h2 id="2-2-配置cgroup-drivers">2.2 配置cgroup drivers</h2>
<p>CentOS7使用的是<code>systemd</code>来初始化系统并管理进程，初始化进程会生成并使用一个 root 控制组 (<code>cgroup</code>), 并充当 <code>cgroup</code> 管理器。 <code>Systemd</code> 与 <code>cgroup</code> 集成紧密，并将为每个 <code>systemd</code> 单元分配一个 <code>cgroup</code>。 我们也可以配置<code>容器运行时</code>和 <code>kubelet</code> 使用 <code>cgroupfs</code>。 连同 <code>systemd</code> 一起使用 <code>cgroupfs</code> 意味着将有两个不同的 <code>cgroup 管理器</code>。而当一个系统中同时存在cgroupfs和systemd两者时，容易变得不稳定，因此最好更改设置，令容器运行时和 kubelet 使用 <code>systemd</code> 作为 <code>cgroup</code> 驱动，以此使系统更为稳定。 对于<code>containerd</code>, 需要设置配置文件<code>/etc/containerd/config.toml</code>中的 <code>SystemdCgroup</code> 参数。</p>
<blockquote>
<p>参考k8s官方的说明文档：</p>
<p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd-systemd">https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd-systemd</a></p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">[plugins.<span class="hljs-string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc]<br>  ...<br>  [plugins.<span class="hljs-string">&quot;io.containerd.grpc.v1.cri&quot;</span>.containerd.runtimes.runc.options]<br>    SystemdCgroup = <span class="hljs-literal">true</span><br></code></pre></div></td></tr></table></figure>
<p>接下来我们开始配置containerd的cgroup driver</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 查看默认的配置文件，我们可以看到是没有启用systemd</span><br>$ containerd config default | grep SystemdCgroup<br>            SystemdCgroup = <span class="hljs-literal">false</span><br>            <br><span class="hljs-comment"># 使用yum安装的containerd的配置文件非常简单</span><br>$ <span class="hljs-built_in">cat</span> /etc/containerd/config.toml | egrep -v <span class="hljs-string">&quot;^#|^$&quot;</span><br>disabled_plugins = [<span class="hljs-string">&quot;cri&quot;</span>]<br><br><span class="hljs-comment"># 导入一个完整版的默认配置文件模板为config.toml</span><br>$ <span class="hljs-built_in">mv</span> /etc/containerd/config.toml /etc/containerd/config.toml.origin<br>$ containerd config default &gt; /etc/containerd/config.toml<br><span class="hljs-comment"># 修改SystemdCgroup参数并重启</span><br>$ sed -i <span class="hljs-string">&#x27;s/SystemdCgroup = false/SystemdCgroup = true/g&#x27;</span> /etc/containerd/config.toml<br>$ systemctl restart containerd<br><span class="hljs-comment"># 重启之后我们再检查配置就会发现已经启用了SystemdCgroup</span><br>$ containerd config dump | grep SystemdCgroup<br>            SystemdCgroup = <span class="hljs-literal">true</span><br><br><span class="hljs-comment"># 查看containerd状态的时候我们可以看到cni相关的报错</span><br><span class="hljs-comment"># 这是因为我们先安装了cni-plugins但是还没有安装k8s的cni插件</span><br><span class="hljs-comment"># 属于正常情况</span><br>$ systemctl status containerd -l<br>May 12 09:57:31 tiny-kubeproxy-free-master-18-1.k8s.tcinternal containerd[5758]: time=<span class="hljs-string">&quot;2022-05-12T09:57:31.100285056+08:00&quot;</span> level=error msg=<span class="hljs-string">&quot;failed to load cni during init, please check CRI plugin status before setting up network for pods&quot;</span> error=<span class="hljs-string">&quot;cni config load failed: no network config found in /etc/cni/net.d: cni plugin not initialized: failed to load cni config&quot;</span><br><br></code></pre></div></td></tr></table></figure>
<h2 id="2-3-关于kubelet的cgroup-driver">2.3 关于kubelet的cgroup driver</h2>
<p>k8s官方有<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/">详细的文档</a>介绍了如何设置kubelet的<code>cgroup driver</code>，需要特别注意的是，在1.22版本开始，如果没有手动设置kubelet的cgroup driver，那么默认会设置为systemd</p>
<blockquote>
<p><strong>Note:</strong> In v1.22, if the user is not setting the <code>cgroupDriver</code> field under <code>KubeletConfiguration</code>, <code>kubeadm</code> will default it to <code>systemd</code>.</p>
</blockquote>
<p>一个比较简单的指定kubelet的<code>cgroup driver</code>的方法就是在<code>kubeadm-config.yaml</code>加入<code>cgroupDriver</code>字段</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-comment"># kubeadm-config.yaml</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta3</span><br><span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-string">v1.21.0</span><br><span class="hljs-meta">---</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeletConfiguration</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubelet.config.k8s.io/v1beta1</span><br><span class="hljs-attr">cgroupDriver:</span> <span class="hljs-string">systemd</span><br></code></pre></div></td></tr></table></figure>
<p>我们可以直接查看configmaps来查看初始化之后集群的kubeadm-config配置。</p>
<figure class="highlight asciidoc"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs asciidoc">$ kubectl describe configmaps kubeadm-config -n kube-system<br>Name:         kubeadm-config<br>Namespace:    kube-system<br>Labels:       &lt;none&gt;<br>Annotations:  &lt;none&gt;<br><br><span class="hljs-section">Data</span><br><span class="hljs-section">====</span><br><span class="hljs-section">ClusterConfiguration:</span><br><span class="hljs-section">----</span><br>apiServer:<br><span class="hljs-code">  extraArgs:</span><br><span class="hljs-code">    authorization-mode: Node,RBAC</span><br><span class="hljs-code">  timeoutForControlPlane: 4m0s</span><br>apiVersion: kubeadm.k8s.io/v1beta3<br>certificatesDir: /etc/kubernetes/pki<br>clusterName: kubernetes<br>controllerManager: &#123;&#125;<br>dns: &#123;&#125;<br>etcd:<br><span class="hljs-code">  local:</span><br><span class="hljs-code">    dataDir: /var/lib/etcd</span><br>imageRepository: registry.aliyuncs.com/google_containers<br>kind: ClusterConfiguration<br>kubernetesVersion: v1.23.6<br>networking:<br><span class="hljs-code">  dnsDomain: cali-cluster.tclocal</span><br><span class="hljs-code">  serviceSubnet: 10.88.0.0/18</span><br>scheduler: &#123;&#125;<br><br><br><span class="hljs-section">BinaryData</span><br><span class="hljs-section">====</span><br><br>Events:  &lt;none&gt;<br></code></pre></div></td></tr></table></figure>
<p>当然因为我们需要安装的版本高于1.22.0并且使用的就是<code>systemd</code>，因此可以不用再重复配置。</p>
<h1>3、安装kube三件套</h1>
<blockquote>
<p>对应的官方文档可以参考这里</p>
<p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl</a></p>
</blockquote>
<p>kube三件套就是<code>kubeadm</code>、<code>kubelet</code> 和 <code>kubectl</code>，三者的具体功能和作用如下：</p>
<ul>
<li><code>kubeadm</code>：用来初始化集群的指令。</li>
<li><code>kubelet</code>：在集群中的每个节点上用来启动 Pod 和容器等。</li>
<li><code>kubectl</code>：用来与集群通信的命令行工具。</li>
</ul>
<p>需要注意的是：</p>
<ul>
<li><code>kubeadm</code>不会帮助我们管理<code>kubelet</code>和<code>kubectl</code>，其他两者也是一样的，也就是说这三者是相互独立的，并不存在谁管理谁的情况；</li>
<li><code>kubelet</code>的版本必须小于等于<code>API-server</code>的版本，否则容易出现兼容性的问题；</li>
<li><code>kubectl</code>并不是集群中的每个节点都需要安装，也并不是一定要安装在集群中的节点，可以单独安装在自己本地的机器环境上面，然后配合<code>kubeconfig</code>文件即可使用<code>kubectl</code>命令来远程管理对应的k8s集群；</li>
</ul>
<p>CentOS7的安装比较简单，我们直接使用官方提供的<code>yum</code>源即可。需要注意的是这里需要设置<code>selinux</code>的状态，但是前面我们已经关闭了selinux，因此这里略过这步。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 直接导入谷歌官方的yum源</span><br><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF | sudo tee /etc/yum.repos.d/kubernetes.repo</span><br><span class="hljs-string">[kubernetes]</span><br><span class="hljs-string">name=Kubernetes</span><br><span class="hljs-string">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch</span><br><span class="hljs-string">enabled=1</span><br><span class="hljs-string">gpgcheck=1</span><br><span class="hljs-string">repo_gpgcheck=1</span><br><span class="hljs-string">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg</span><br><span class="hljs-string">exclude=kubelet kubeadm kubectl</span><br><span class="hljs-string">EOF</span><br><br><span class="hljs-comment"># 当然如果连不上谷歌的源，可以考虑使用国内的阿里镜像源</span><br><span class="hljs-built_in">cat</span> &lt;&lt;<span class="hljs-string">EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="hljs-string">[kubernetes]</span><br><span class="hljs-string">name=Kubernetes</span><br><span class="hljs-string">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</span><br><span class="hljs-string">enabled=1</span><br><span class="hljs-string">gpgcheck=1</span><br><span class="hljs-string">repo_gpgcheck=1</span><br><span class="hljs-string">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="hljs-string">EOF</span><br><br><br><span class="hljs-comment"># 接下来直接安装三件套即可</span><br>sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes<br><br><span class="hljs-comment"># 如果网络环境不好出现gpgcheck验证失败导致无法正常读取yum源，可以考虑关闭该yum源的repo_gpgcheck</span><br>sed -i <span class="hljs-string">&#x27;s/repo_gpgcheck=1/repo_gpgcheck=0/g&#x27;</span> /etc/yum.repos.d/kubernetes.repo<br><span class="hljs-comment"># 或者在安装的时候禁用gpgcheck</span><br>sudo yum install -y kubelet kubeadm kubectl --nogpgcheck --disableexcludes=kubernetes<br><br><span class="hljs-comment"># 如果想要安装特定版本，可以使用这个命令查看相关版本的信息</span><br>sudo yum list --nogpgcheck kubelet kubeadm kubectl --showduplicates --disableexcludes=kubernetes<br><br><br><span class="hljs-comment"># 安装完成后配置开机自启kubelet</span><br>sudo systemctl <span class="hljs-built_in">enable</span> --now kubelet<br></code></pre></div></td></tr></table></figure>
<h1>4、初始化集群</h1>
<h2 id="4-0-etcd高可用">4.0 etcd高可用</h2>
<p>etcd高可用架构参考<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/">这篇官方文档</a>，主要可以分为堆叠etcd方案和外置etcd方案，两者的区别就是etcd是否部署在apiserver所在的node机器上面，这里我们主要使用的是堆叠etcd部署方案。</p>
<p><img src="https://resource.tinychen.com/202212091606423.png" srcset="/img/loading.gif" lazyload alt=""></p>
<p><img src="https://resource.tinychen.com/202212091606949.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h2 id="4-1-apiserver高可用">4.1 apiserver高可用</h2>
<p>apisever高可用配置参考<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#high-availability-considerations">这篇官方文档</a>。目前apiserver的高可用比较主流的<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#keepalived-and-haproxy">官方推荐方案</a>是使用keepalived和haproxy，由于centos7自带的版本较旧，重新编译又过于麻烦，因此我们可以参考官方给出的<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#option-2-run-the-services-as-static-pods">静态pod的部署方式</a>，提前将相关的配置文件放置到<code>/etc/kubernetes/manifests</code>目录下即可(需要提前手动创建好目录)。官方表示对于我们这种堆叠部署控制面master节点和etcd的方式而言这是一种优雅的解决方案。</p>
<blockquote>
<p>This is an elegant solution, in particular with the setup described under <a target="_blank" rel="noopener" href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#stacked-control-plane-and-etcd-nodes">Stacked control plane and etcd nodes</a>.</p>
</blockquote>
<p>首先我们需要准备好三台master节点上面的keepalived配置文件和haproxy配置文件：</p>
<figure class="highlight dust"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs dust"><span class="language-xml">! /etc/keepalived/keepalived.conf</span><br><span class="language-xml">! Configuration File for keepalived</span><br><span class="language-xml">global_defs </span><span class="hljs-template-variable">&#123;</span><br><span class="hljs-template-variable">    router_id LVS_DEVEL</span><br><span class="hljs-template-variable">&#125;</span><span class="language-xml"></span><br><span class="language-xml">vrrp_script check_apiserver </span><span class="hljs-template-variable">&#123;</span><br><span class="hljs-template-variable">  script &quot;/etc/keepalived/check_apiserver.sh&quot;</span><br><span class="hljs-template-variable">  interval 3</span><br><span class="hljs-template-variable">  weight -2</span><br><span class="hljs-template-variable">  fall 10</span><br><span class="hljs-template-variable">  rise 2</span><br><span class="hljs-template-variable">&#125;</span><span class="language-xml"></span><br><span class="language-xml"></span><br><span class="language-xml">vrrp_instance VI_1 </span><span class="hljs-template-variable">&#123;</span><br><span class="hljs-template-variable">    state $&#123;STATE&#125;</span><span class="language-xml"></span><br><span class="language-xml">    interface $</span><span class="hljs-template-variable">&#123;INTERFACE&#125;</span><span class="language-xml"></span><br><span class="language-xml">    virtual_router_id $</span><span class="hljs-template-variable">&#123;ROUTER_ID&#125;</span><span class="language-xml"></span><br><span class="language-xml">    priority $</span><span class="hljs-template-variable">&#123;PRIORITY&#125;</span><span class="language-xml"></span><br><span class="language-xml">    authentication </span><span class="hljs-template-variable">&#123;</span><br><span class="hljs-template-variable">        auth_type PASS</span><br><span class="hljs-template-variable">        auth_pass $&#123;AUTH_PASS&#125;</span><span class="language-xml"></span><br><span class="language-xml">    &#125;</span><br><span class="language-xml">    virtual_ipaddress </span><span class="hljs-template-variable">&#123;</span><br><span class="hljs-template-variable">        $&#123;APISERVER_VIP&#125;</span><span class="language-xml"></span><br><span class="language-xml">    &#125;</span><br><span class="language-xml">    track_script </span><span class="hljs-template-variable">&#123;</span><br><span class="hljs-template-variable">        check_apiserver</span><br><span class="hljs-template-variable">    &#125;</span><span class="language-xml"></span><br><span class="language-xml">&#125;</span><br></code></pre></div></td></tr></table></figure>
<p>实际上我们需要区分三台控制面节点的状态</p>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript">! /etc/keepalived/keepalived.conf<br>! Configuration File <span class="hljs-keyword">for</span> keepalived<br>global_defs &#123;<br>    router_id CILIUM_MASTER_80_1<br>&#125;<br>vrrp_script check_apiserver &#123;<br>  script <span class="hljs-string">&quot;/etc/keepalived/check_apiserver.sh&quot;</span><br>  <span class="hljs-built_in">int</span>erval <span class="hljs-number">3</span><br>  weight <span class="hljs-number">-2</span><br>  fall <span class="hljs-number">10</span><br>  rise <span class="hljs-number">2</span><br>&#125;<br><br>vrrp_instance cilium_ha_apiserver_10_31_80_0 &#123;<br>    state MASTER<br>    <span class="hljs-keyword">interface</span> <span class="hljs-symbol">eth0</span><br>    <span class="hljs-symbol">virtual_router_id</span> <span class="hljs-symbol">80</span><br>    <span class="hljs-symbol">priority</span> <span class="hljs-symbol">100</span><br>    <span class="hljs-symbol">authentication</span> &#123;<br>        auth_type PASS<br>        auth_pass <span class="hljs-symbol">pass@</span><span class="hljs-number">77</span><br>    &#125;<br>    virtual_ipaddress &#123;<br>        <span class="hljs-number">10.31</span><span class="hljs-number">.80</span><span class="hljs-number">.0</span><br>    &#125;<br>    track_script &#123;<br>        check_apiserver<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript">! /etc/keepalived/keepalived.conf<br>! Configuration File <span class="hljs-keyword">for</span> keepalived<br>global_defs &#123;<br>    router_id CILIUM_MASTER_80_2<br>&#125;<br>vrrp_script check_apiserver &#123;<br>  script <span class="hljs-string">&quot;/etc/keepalived/check_apiserver.sh&quot;</span><br>  <span class="hljs-built_in">int</span>erval <span class="hljs-number">3</span><br>  weight <span class="hljs-number">-2</span><br>  fall <span class="hljs-number">10</span><br>  rise <span class="hljs-number">2</span><br>&#125;<br><br>vrrp_instance cilium_ha_apiserver_10_31_80_0 &#123;<br>    state BACKUP<br>    <span class="hljs-keyword">interface</span> <span class="hljs-symbol">eth0</span><br>    <span class="hljs-symbol">virtual_router_id</span> <span class="hljs-symbol">80</span><br>    <span class="hljs-symbol">priority</span> <span class="hljs-symbol">90</span><br>    <span class="hljs-symbol">authentication</span> &#123;<br>        auth_type PASS<br>        auth_pass <span class="hljs-symbol">pass@</span><span class="hljs-number">77</span><br>    &#125;<br>    virtual_ipaddress &#123;<br>        <span class="hljs-number">10.31</span><span class="hljs-number">.80</span><span class="hljs-number">.0</span><br>    &#125;<br>    track_script &#123;<br>        check_apiserver<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript">! /etc/keepalived/keepalived.conf<br>! Configuration File <span class="hljs-keyword">for</span> keepalived<br>global_defs &#123;<br>    router_id CILIUM_MASTER_80_3<br>&#125;<br>vrrp_script check_apiserver &#123;<br>  script <span class="hljs-string">&quot;/etc/keepalived/check_apiserver.sh&quot;</span><br>  <span class="hljs-built_in">int</span>erval <span class="hljs-number">3</span><br>  weight <span class="hljs-number">-2</span><br>  fall <span class="hljs-number">10</span><br>  rise <span class="hljs-number">2</span><br>&#125;<br><br>vrrp_instance cilium_ha_apiserver_10_31_80_0 &#123;<br>    state BACKUP<br>    <span class="hljs-keyword">interface</span> <span class="hljs-symbol">eth0</span><br>    <span class="hljs-symbol">virtual_router_id</span> <span class="hljs-symbol">80</span><br>    <span class="hljs-symbol">priority</span> <span class="hljs-symbol">80</span><br>    <span class="hljs-symbol">authentication</span> &#123;<br>        auth_type PASS<br>        auth_pass <span class="hljs-symbol">pass@</span><span class="hljs-number">77</span><br>    &#125;<br>    virtual_ipaddress &#123;<br>        <span class="hljs-number">10.31</span><span class="hljs-number">.80</span><span class="hljs-number">.0</span><br>    &#125;<br>    track_script &#123;<br>        check_apiserver<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure>
<p>这是haproxy的配置文件模板：</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># /etc/haproxy/haproxy.cfg</span><br><span class="hljs-comment">#---------------------------------------------------------------------</span><br><span class="hljs-comment"># Global settings</span><br><span class="hljs-comment">#---------------------------------------------------------------------</span><br>global<br>    <span class="hljs-built_in">log</span> /dev/log local0<br>    <span class="hljs-built_in">log</span> /dev/log local1 notice<br>    daemon<br><br><span class="hljs-comment">#---------------------------------------------------------------------</span><br><span class="hljs-comment"># common defaults that all the &#x27;listen&#x27; and &#x27;backend&#x27; sections will</span><br><span class="hljs-comment"># use if not designated in their block</span><br><span class="hljs-comment">#---------------------------------------------------------------------</span><br>defaults<br>    mode                    http<br>    <span class="hljs-built_in">log</span>                     global<br>    option                  httplog<br>    option                  dontlognull<br>    option http-server-close<br>    option forwardfor       except 127.0.0.0/8<br>    option                  redispatch<br>    retries                 1<br>    <span class="hljs-built_in">timeout</span> http-request    10s<br>    <span class="hljs-built_in">timeout</span> queue           20s<br>    <span class="hljs-built_in">timeout</span> connect         5s<br>    <span class="hljs-built_in">timeout</span> client          20s<br>    <span class="hljs-built_in">timeout</span> server          20s<br>    <span class="hljs-built_in">timeout</span> http-keep-alive 10s<br>    <span class="hljs-built_in">timeout</span> check           10s<br><br><span class="hljs-comment">#---------------------------------------------------------------------</span><br><span class="hljs-comment"># apiserver frontend which proxys to the control plane nodes</span><br><span class="hljs-comment">#---------------------------------------------------------------------</span><br>frontend apiserver<br>    <span class="hljs-built_in">bind</span> *:<span class="hljs-variable">$&#123;APISERVER_DEST_PORT&#125;</span><br>    mode tcp<br>    option tcplog<br>    default_backend apiserver<br><br><span class="hljs-comment">#---------------------------------------------------------------------</span><br><span class="hljs-comment"># round robin balancing for apiserver</span><br><span class="hljs-comment">#---------------------------------------------------------------------</span><br>backend apiserver<br>    option httpchk GET /healthz<br>    http-check expect status 200<br>    mode tcp<br>    option ssl-hello-chk<br>    balance     roundrobin<br>        server <span class="hljs-variable">$&#123;HOST1_ID&#125;</span> <span class="hljs-variable">$&#123;HOST1_ADDRESS&#125;</span>:<span class="hljs-variable">$&#123;APISERVER_SRC_PORT&#125;</span> check<br>        <span class="hljs-comment"># [...]</span><br></code></pre></div></td></tr></table></figure>
<p>这是keepalived的检测脚本，注意这里的<code>$&#123;APISERVER_VIP&#125;</code>和<code>$&#123;APISERVER_DEST_PORT&#125;</code>要替换为集群的实际VIP和端口</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/sh</span><br>APISERVER_VIP=&quot;10.31.80.0&quot;<br>APISERVER_DEST_PORT=&quot;8443&quot;<br><br>errorExit() &#123;<br>    echo &quot;*** $*&quot; 1&gt;&amp;2<br>    exit 1<br>&#125;<br><br>curl --silent --max-time 2 --insecure https://localhost:$&#123;APISERVER_DEST_PORT&#125;/ -o /dev/null || errorExit &quot;Error GET https://localhost:$&#123;APISERVER_DEST_PORT&#125;/&quot;<br>if ip addr | grep -q $&#123;APISERVER_VIP&#125;; then<br>    curl --silent --max-time 2 --insecure https://$&#123;APISERVER_VIP&#125;:$&#123;APISERVER_DEST_PORT&#125;/ -o /dev/null || errorExit &quot;Error GET https://$&#123;APISERVER_VIP&#125;:$&#123;APISERVER_DEST_PORT&#125;/&quot;<br>fi<br></code></pre></div></td></tr></table></figure>
<p>这是keepalived的部署文件<code>/etc/kubernetes/manifests/keepalived.yaml</code>，注意这里的配置文件路径要和上面的对应一致。</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">creationTimestamp:</span> <span class="hljs-literal">null</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">keepalived</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">osixia/keepalived:2.0.17</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">keepalived</span><br>    <span class="hljs-attr">resources:</span> &#123;&#125;<br>    <span class="hljs-attr">securityContext:</span><br>      <span class="hljs-attr">capabilities:</span><br>        <span class="hljs-attr">add:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">NET_ADMIN</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">NET_BROADCAST</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-string">NET_RAW</span><br>    <span class="hljs-attr">volumeMounts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/usr/local/etc/keepalived/keepalived.conf</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">config</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/etc/keepalived/check_apiserver.sh</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">check</span><br>  <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">volumes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span><br>      <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/keepalived/keepalived.conf</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">config</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span><br>      <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/keepalived/check_apiserver.sh</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">check</span><br><span class="hljs-attr">status:</span> &#123;&#125;<br></code></pre></div></td></tr></table></figure>
<p>这是haproxy的部署文件<code>/etc/kubernetes/manifests/haproxy.yaml</code>，注意这里的配置文件路径要和上面的对应一致，且<code>$&#123;APISERVER_DEST_PORT&#125;</code>要换成我们对应的apiserver的端口，这里我们改为8443，避免和原有的6443端口冲突</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">haproxy</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">containers:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">haproxy:2.1.4</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">haproxy</span><br>    <span class="hljs-attr">livenessProbe:</span><br>      <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">8</span><br>      <span class="hljs-attr">httpGet:</span><br>        <span class="hljs-attr">host:</span> <span class="hljs-string">localhost</span><br>        <span class="hljs-attr">path:</span> <span class="hljs-string">/healthz</span><br>        <span class="hljs-comment">#port: $&#123;APISERVER_DEST_PORT&#125;</span><br>        <span class="hljs-attr">port:</span> <span class="hljs-number">8443</span><br>        <span class="hljs-attr">scheme:</span> <span class="hljs-string">HTTPS</span><br>    <span class="hljs-attr">volumeMounts:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/usr/local/etc/haproxy/haproxy.cfg</span><br>      <span class="hljs-attr">name:</span> <span class="hljs-string">haproxyconf</span><br>      <span class="hljs-attr">readOnly:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">volumes:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">hostPath:</span><br>      <span class="hljs-attr">path:</span> <span class="hljs-string">/etc/haproxy/haproxy.cfg</span><br>      <span class="hljs-attr">type:</span> <span class="hljs-string">FileOrCreate</span><br>    <span class="hljs-attr">name:</span> <span class="hljs-string">haproxyconf</span><br><span class="hljs-attr">status:</span> &#123;&#125;<br></code></pre></div></td></tr></table></figure>
<h2 id="4-2-编写配置文件">4.2 编写配置文件</h2>
<p>在集群中所有节点都执行完上面的操作之后，我们就可以开始创建k8s集群了。<strong>因为我们这次需要进行高可用部署，所以初始化的时候先挑任意一台master控制面节点进行操作即可。</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 我们先使用kubeadm命令查看一下主要的几个镜像版本</span><br>$ kubeadm config images list<br>registry.k8s.io/kube-apiserver:v1.25.4<br>registry.k8s.io/kube-controller-manager:v1.25.4<br>registry.k8s.io/kube-scheduler:v1.25.4<br>registry.k8s.io/kube-proxy:v1.25.4<br>registry.k8s.io/pause:3.8<br>registry.k8s.io/etcd:3.5.5-0<br>registry.k8s.io/coredns/coredns:v1.9.3<br><br><span class="hljs-comment"># 为了方便编辑和管理，我们还是把初始化参数导出成配置文件</span><br>$ kubeadm config <span class="hljs-built_in">print</span> init-defaults &gt; kubeadm-cilium-ha.conf<br><br></code></pre></div></td></tr></table></figure>
<ul>
<li>考虑到大多数情况下国内的网络无法使用谷歌的镜像源(1.25版本开始从<code>k8s.gcr.io</code>换为<code>registry.k8s.io</code>)，我们可以直接在配置文件中修改<code>imageRepository</code>参数为阿里的镜像源<code>registry.aliyuncs.com/google_containers</code></li>
<li><code>kubernetesVersion</code>字段用来指定我们要安装的k8s版本</li>
<li><code>localAPIEndpoint</code>参数需要修改为我们的master节点的IP和端口，初始化之后的k8s集群的apiserver地址就是这个</li>
<li><code>criSocket</code>从1.24.0版本开始已经默认变成了<code>containerd</code></li>
<li><code>podSubnet</code>、<code>serviceSubnet</code>和<code>dnsDomain</code>两个参数默认情况下可以不用修改，这里我按照自己的需求进行了变更</li>
<li><code>nodeRegistration</code>里面的<code>name</code>参数修改为对应master节点的<code>hostname</code></li>
<li><code>controlPlaneEndpoint</code>参数配置的才是我们前面配置的集群高可用apiserver的地址</li>
<li>新增配置块使用ipvs，具体可以参考<a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md#cluster-created-by-kubeadm">官方文档</a></li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta3</span><br><span class="hljs-attr">bootstrapTokens:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">groups:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">system:bootstrappers:kubeadm:default-node-token</span><br>  <span class="hljs-attr">token:</span> <span class="hljs-string">abcdef.0123456789abcdef</span><br>  <span class="hljs-attr">ttl:</span> <span class="hljs-string">24h0m0s</span><br>  <span class="hljs-attr">usages:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">signing</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">authentication</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">InitConfiguration</span><br><span class="hljs-attr">localAPIEndpoint:</span><br>  <span class="hljs-attr">advertiseAddress:</span> <span class="hljs-number">10.31</span><span class="hljs-number">.80</span><span class="hljs-number">.1</span><br>  <span class="hljs-attr">bindPort:</span> <span class="hljs-number">6443</span><br><span class="hljs-attr">nodeRegistration:</span><br>  <span class="hljs-attr">criSocket:</span> <span class="hljs-string">unix:///var/run/containerd/containerd.sock</span><br>  <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">k8s-cilium-master-10-31-80-1.tinychen.io</span><br>  <span class="hljs-attr">taints:</span> <span class="hljs-literal">null</span><br><span class="hljs-meta">---</span><br><span class="hljs-attr">apiServer:</span><br>  <span class="hljs-attr">timeoutForControlPlane:</span> <span class="hljs-string">4m0s</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeadm.k8s.io/v1beta3</span><br><span class="hljs-attr">certificatesDir:</span> <span class="hljs-string">/etc/kubernetes/pki</span><br><span class="hljs-attr">clusterName:</span> <span class="hljs-string">kubernetes</span><br><span class="hljs-attr">controllerManager:</span> &#123;&#125;<br><span class="hljs-attr">dns:</span> &#123;&#125;<br><span class="hljs-attr">etcd:</span><br>  <span class="hljs-attr">local:</span><br>    <span class="hljs-attr">dataDir:</span> <span class="hljs-string">/var/lib/etcd</span><br><span class="hljs-attr">imageRepository:</span> <span class="hljs-string">registry.aliyuncs.com/google_containers</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ClusterConfiguration</span><br><span class="hljs-attr">kubernetesVersion:</span> <span class="hljs-number">1.25</span><span class="hljs-number">.4</span><br><span class="hljs-attr">controlPlaneEndpoint:</span> <span class="hljs-string">&quot;k8s-cilium-apiserver.tinychen.io:8443&quot;</span><br><span class="hljs-attr">networking:</span><br>  <span class="hljs-attr">dnsDomain:</span> <span class="hljs-string">cili-cluster.tclocal</span><br>  <span class="hljs-attr">serviceSubnet:</span> <span class="hljs-number">10.32</span><span class="hljs-number">.128</span><span class="hljs-number">.0</span><span class="hljs-string">/18</span><br>  <span class="hljs-attr">podSubnet:</span> <span class="hljs-number">10.32</span><span class="hljs-number">.0</span><span class="hljs-number">.0</span><span class="hljs-string">/17</span><br><span class="hljs-attr">scheduler:</span> &#123;&#125;<br><span class="hljs-meta">---</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kubeproxy.config.k8s.io/v1alpha1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">KubeProxyConfiguration</span><br><span class="hljs-attr">mode:</span> <span class="hljs-string">ipvs</span><br><br><br></code></pre></div></td></tr></table></figure>
<h2 id="4-3-初始化集群">4.3 初始化集群</h2>
<p>此时我们再查看对应的配置文件中的镜像版本，就会发现已经变成了对应阿里云镜像源的版本</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 查看一下对应的镜像版本，确定配置文件是否生效</span><br>$ kubeadm config images list --config kubeadm-cilium-ha.conf<br>registry.aliyuncs.com/google_containers/kube-apiserver:v1.25.4<br>registry.aliyuncs.com/google_containers/kube-controller-manager:v1.25.4<br>registry.aliyuncs.com/google_containers/kube-scheduler:v1.25.4<br>registry.aliyuncs.com/google_containers/kube-proxy:v1.25.4<br>registry.aliyuncs.com/google_containers/pause:3.8<br>registry.aliyuncs.com/google_containers/etcd:3.5.5-0<br>registry.aliyuncs.com/google_containers/coredns:v1.9.3<br><br><span class="hljs-comment"># 确认没问题之后我们直接拉取镜像</span><br>$ kubeadm config images pull --config kubeadm-cilium-ha.conf<br>[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.25.4<br>[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.25.4<br>[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.25.4<br>[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.25.4<br>[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.8<br>[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.5.5-0<br>[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.9.3<br><br><span class="hljs-comment"># 初始化，注意添加参数--upload-certs确保证书能够上传到kubernetes集群中以secret保存</span><br>$ kubeadm init --config kubeadm-cilium-ha.conf  --upload-certs<br>[init] Using Kubernetes version: v1.25.4<br>[preflight] Running pre-flight checks<br>        [WARNING Service-Kubelet]: kubelet service is not enabled, please run <span class="hljs-string">&#x27;systemctl enable kubelet.service&#x27;</span><br>[preflight] Pulling images required <span class="hljs-keyword">for</span> setting up a Kubernetes cluster<br>[preflight] This might take a minute or two, depending on the speed of your internet connection<br>[preflight] You can also perform this action <span class="hljs-keyword">in</span> beforehand using <span class="hljs-string">&#x27;kubeadm config images pull&#x27;</span><br>...此处略去一堆输出...<br></code></pre></div></td></tr></table></figure>
<p>当我们看到下面这个输出结果的时候，我们的集群就算是初始化成功了。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">Your Kubernetes control-plane has initialized successfully!<br><br>To start using your cluster, you need to run the following as a regular user:<br><br>  <span class="hljs-built_in">mkdir</span> -p <span class="hljs-variable">$HOME</span>/.kube<br>  sudo <span class="hljs-built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config<br>  sudo <span class="hljs-built_in">chown</span> $(<span class="hljs-built_in">id</span> -u):$(<span class="hljs-built_in">id</span> -g) <span class="hljs-variable">$HOME</span>/.kube/config<br><br>Alternatively, <span class="hljs-keyword">if</span> you are the root user, you can run:<br><br>  <span class="hljs-built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf<br><br>You should now deploy a pod network to the cluster.<br>Run <span class="hljs-string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:<br>  https://kubernetes.io/docs/concepts/cluster-administration/addons/<br><br>You can now <span class="hljs-built_in">join</span> any number of the control-plane node running the following <span class="hljs-built_in">command</span> on each as root:<br><br>  kubeadm <span class="hljs-built_in">join</span> k8s-cilium-apiserver.tinychen.io:6443 --token abcdef.0123456789abcdef \<br>        --discovery-token-ca-cert-hash sha256:cc68219b233262d8834ad5d6e96166be487c751b53fb9ec19a5ca3599b538a33 \<br>        --control-plane --certificate-key e0bfc81fd9277731611f4b4351beed53a1d0c4e1c82932734a38919ddd76a185<br><br>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!<br>As a safeguard, uploaded-certs will be deleted <span class="hljs-keyword">in</span> two hours; If necessary, you can use<br><span class="hljs-string">&quot;kubeadm init phase upload-certs --upload-certs&quot;</span> to reload certs afterward.<br><br>Then you can <span class="hljs-built_in">join</span> any number of worker nodes by running the following on each as root:<br><br>kubeadm <span class="hljs-built_in">join</span> k8s-cilium-apiserver.tinychen.io:6443 --token abcdef.0123456789abcdef \<br>        --discovery-token-ca-cert-hash sha256:cc68219b233262d8834ad5d6e96166be487c751b53fb9ec19a5ca3599b538a33<br></code></pre></div></td></tr></table></figure>
<p>接下来我们在剩下的两个master节点上面执行上面输出的命令，注意要执行带有<code>--control-plane --certificate-key</code>这两个参数的命令，其中<code>--control-plane</code>参数是确定该节点为master控制面节点，而<code>--certificate-key</code>参数则是把我们前面初始化集群的时候通过<code>--upload-certs</code>上传到k8s集群中的证书下载下来使用。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs livecodeserver">This node has joined <span class="hljs-keyword">the</span> cluster <span class="hljs-keyword">and</span> <span class="hljs-keyword">a</span> <span class="hljs-built_in">new</span> control plane instance was created:<br><br>* Certificate signing request was sent <span class="hljs-built_in">to</span> apiserver <span class="hljs-keyword">and</span> approval was received.<br>* The Kubelet was informed <span class="hljs-keyword">of</span> <span class="hljs-keyword">the</span> <span class="hljs-built_in">new</span> secure connection details.<br>* Control plane label <span class="hljs-keyword">and</span> taint were applied <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> <span class="hljs-built_in">new</span> node.<br>* The Kubernetes control plane instances scaled up.<br>* A <span class="hljs-built_in">new</span> etcd member was added <span class="hljs-built_in">to</span> <span class="hljs-keyword">the</span> <span class="hljs-built_in">local</span>/stacked etcd cluster.<br><br>To <span class="hljs-built_in">start</span> administering your cluster <span class="hljs-built_in">from</span> this node, you need <span class="hljs-built_in">to</span> run <span class="hljs-keyword">the</span> following <span class="hljs-keyword">as</span> <span class="hljs-keyword">a</span> regular user:<br><br>        mkdir -p $HOME/.kube<br>        sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>        sudo chown $(id -u):$(id -g) $HOME/.kube/config<br><br>Run <span class="hljs-string">&#x27;kubectl get nodes&#x27;</span> <span class="hljs-built_in">to</span> see this node join <span class="hljs-keyword">the</span> cluster.<br></code></pre></div></td></tr></table></figure>
<p>最后再对剩下的三个worker节点执行普通的加入集群命令，当看到下面的输出的时候说明节点成功加入集群了。</p>
<figure class="highlight gams"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs gams">This node has joined the cluster:<br><span class="hljs-comment">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="hljs-comment">* The Kubelet was informed of the new secure connection details.</span><br><br>Run <span class="hljs-string">&#x27;kubectl get nodes&#x27;</span> on the control-plane to see this node join the cluster.<br></code></pre></div></td></tr></table></figure>
<p>如果不小心没保存初始化成功的输出信息，或者是以后还需要新增节点也没有关系，我们可以使用kubectl工具查看或者生成token</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">查看现有的token列表</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">kubeadm token list</span><br>TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS<br>abcdef.0123456789abcdef   23h         2022-12-09T08:14:37Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token<br>dss91p.3r5don4a3e9r2f29   1h          2022-12-08T10:14:36Z   &lt;none&gt;                   Proxy for managing TTL for the kubeadm-certs secret        &lt;none&gt;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">如果token已经失效，那就再创建一个新的token</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">kubeadm token create</span><br>8hmoux.jabpgvs521r8rsqm<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">kubeadm token list</span><br>TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS<br>8hmoux.jabpgvs521r8rsqm   23h         2022-12-09T08:29:29Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token<br>abcdef.0123456789abcdef   23h         2022-12-09T08:14:37Z   authentication,signing   &lt;none&gt;                                                     system:bootstrappers:kubeadm:default-node-token<br>dss91p.3r5don4a3e9r2f29   1h          2022-12-08T10:14:36Z   &lt;none&gt;                   Proxy for managing TTL for the kubeadm-certs secret        &lt;none&gt;<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">如果找不到--discovery-token-ca-cert-hash参数，则可以在master节点上使用openssl工具来获取</span><br><span class="hljs-meta prompt_">$ </span><span class="language-bash">openssl x509 -pubkey -<span class="hljs-keyword">in</span> /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed <span class="hljs-string">&#x27;s/^.* //&#x27;</span></span><br>cc68219b233262d8834ad5d6e96166be487c751b53fb9ec19a5ca3599b538a33<br></code></pre></div></td></tr></table></figure>
<h2 id="4-4-配置kubeconfig">4.4 配置kubeconfig</h2>
<p>刚初始化成功之后，我们还没办法马上查看k8s集群信息，需要配置kubeconfig相关参数才能正常使用kubectl连接apiserver读取集群信息。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 对于非root用户，可以这样操作</span><br><span class="hljs-built_in">mkdir</span> -p <span class="hljs-variable">$HOME</span>/.kube<br>sudo <span class="hljs-built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="hljs-variable">$HOME</span>/.kube/config<br>sudo <span class="hljs-built_in">chown</span> $(<span class="hljs-built_in">id</span> -u):$(<span class="hljs-built_in">id</span> -g) <span class="hljs-variable">$HOME</span>/.kube/config<br><br><span class="hljs-comment"># 如果是root用户，可以直接导入环境变量</span><br><span class="hljs-built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf<br><br><span class="hljs-comment"># 添加kubectl的自动补全功能</span><br><span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;source &lt;(kubectl completion bash)&quot;</span> &gt;&gt; ~/.bashrc<br></code></pre></div></td></tr></table></figure>
<blockquote>
<p>前面我们提到过<code>kubectl</code>不一定要安装在集群内，实际上只要是任何一台能连接到<code>apiserver</code>的机器上面都可以安装<code>kubectl</code>并且根据步骤配置<code>kubeconfig</code>，就可以使用<code>kubectl</code>命令行来管理对应的k8s集群。</p>
</blockquote>
<p>配置完成后，我们再执行相关命令就可以查看集群的信息了，但是此时节点的状态还是<code>NotReady</code>，接下来就需要部署CNI了。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl cluster-info<br>Kubernetes control plane is running at https://k8s-cilium-apiserver.tinychen.io:8443<br>CoreDNS is running at https://k8s-cilium-apiserver.tinychen.io:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy<br><br>To further debug and diagnose cluster problems, use <span class="hljs-string">&#x27;kubectl cluster-info dump&#x27;</span>.<br><br><br>$ kubectl get nodes -o wide<br>NAME                                       STATUS     ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME<br>k8s-cilium-master-10-31-80-1.tinychen.io   NotReady   control-plane   16m     v1.25.4   10.31.80.1    &lt;none&gt;        CentOS Linux 7 (Core)   6.0.11-1.el7.elrepo.x86_64   containerd://1.6.11<br>k8s-cilium-master-10-31-80-2.tinychen.io   NotReady   control-plane   12m     v1.25.4   10.31.80.2    &lt;none&gt;        CentOS Linux 7 (Core)   6.0.11-1.el7.elrepo.x86_64   containerd://1.6.11<br>k8s-cilium-master-10-31-80-3.tinychen.io   NotReady   control-plane   7m42s   v1.25.4   10.31.80.3    &lt;none&gt;        CentOS Linux 7 (Core)   6.0.11-1.el7.elrepo.x86_64   containerd://1.6.11<br>k8s-cilium-worker-10-31-80-4.tinychen.io   NotReady   &lt;none&gt;          5m28s   v1.25.4   10.31.80.4    &lt;none&gt;        CentOS Linux 7 (Core)   6.0.11-1.el7.elrepo.x86_64   containerd://1.6.11<br>k8s-cilium-worker-10-31-80-5.tinychen.io   NotReady   &lt;none&gt;          4m40s   v1.25.4   10.31.80.5    &lt;none&gt;        CentOS Linux 7 (Core)   6.0.11-1.el7.elrepo.x86_64   containerd://1.6.11<br>k8s-cilium-worker-10-31-80-6.tinychen.io   NotReady   &lt;none&gt;          4m9s    v1.25.4   10.31.80.6    &lt;none&gt;        CentOS Linux 7 (Core)   6.0.11-1.el7.elrepo.x86_64   containerd://1.6.11<br><br>$ kubectl get pods -A -o wide<br>NAMESPACE     NAME                                                               READY   STATUS    RESTARTS       AGE   IP           NODE                                       NOMINATED NODE   READINESS GATES<br>kube-system   coredns-c676cc86f-6jg9b                                            0/1     Pending   0              31m   &lt;none&gt;       &lt;none&gt;                                     &lt;none&gt;           &lt;none&gt;<br>kube-system   coredns-c676cc86f-qmx9s                                            0/1     Pending   0              31m   &lt;none&gt;       &lt;none&gt;                                     &lt;none&gt;           &lt;none&gt;<br>kube-system   etcd-k8s-cilium-master-10-31-80-1.tinychen.io                      1/1     Running   0              31m   10.31.80.1   k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   etcd-k8s-cilium-master-10-31-80-2.tinychen.io                      1/1     Running   0              28m   10.31.80.2   k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   etcd-k8s-cilium-master-10-31-80-3.tinychen.io                      1/1     Running   0              22m   10.31.80.3   k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   haproxy-k8s-cilium-master-10-31-80-1.tinychen.io                   1/1     Running   12 (10m ago)   31m   10.31.80.1   k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   haproxy-k8s-cilium-master-10-31-80-2.tinychen.io                   1/1     Running   9 (11m ago)    28m   10.31.80.2   k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   haproxy-k8s-cilium-master-10-31-80-3.tinychen.io                   1/1     Running   8 (11m ago)    22m   10.31.80.3   k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   keepalived-k8s-cilium-master-10-31-80-1.tinychen.io                1/1     Running   0              31m   10.31.80.1   k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   keepalived-k8s-cilium-master-10-31-80-2.tinychen.io                1/1     Running   0              27m   10.31.80.2   k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   keepalived-k8s-cilium-master-10-31-80-3.tinychen.io                1/1     Running   0              22m   10.31.80.3   k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-apiserver-k8s-cilium-master-10-31-80-1.tinychen.io            1/1     Running   0              31m   10.31.80.1   k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-apiserver-k8s-cilium-master-10-31-80-2.tinychen.io            1/1     Running   0              27m   10.31.80.2   k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-apiserver-k8s-cilium-master-10-31-80-3.tinychen.io            1/1     Running   0              22m   10.31.80.3   k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-controller-manager-k8s-cilium-master-10-31-80-1.tinychen.io   1/1     Running   1 (28m ago)    31m   10.31.80.1   k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-controller-manager-k8s-cilium-master-10-31-80-2.tinychen.io   1/1     Running   0              28m   10.31.80.2   k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-controller-manager-k8s-cilium-master-10-31-80-3.tinychen.io   1/1     Running   0              22m   10.31.80.3   k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-52zmk                                                   1/1     Running   0              31m   10.31.80.1   k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-dg4nt                                                   1/1     Running   0              20m   10.31.80.5   k8s-cilium-worker-10-31-80-5.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-gbgr5                                                   1/1     Running   0              28m   10.31.80.2   k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-gpzxv                                                   1/1     Running   0              19m   10.31.80.6   k8s-cilium-worker-10-31-80-6.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-kn9gq                                                   1/1     Running   0              23m   10.31.80.3   k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-xw8nl                                                   1/1     Running   0              20m   10.31.80.4   k8s-cilium-worker-10-31-80-4.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-scheduler-k8s-cilium-master-10-31-80-1.tinychen.io            1/1     Running   1 (28m ago)    31m   10.31.80.1   k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-scheduler-k8s-cilium-master-10-31-80-2.tinychen.io            1/1     Running   0              28m   10.31.80.2   k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-scheduler-k8s-cilium-master-10-31-80-3.tinychen.io            1/1     Running   0              22m   10.31.80.3   k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br><br></code></pre></div></td></tr></table></figure>
<h1>5、安装CNI</h1>
<h2 id="5-1-部署helm3">5.1 部署helm3</h2>
<p>cilium的部署依赖helm3，因此我们在部署cilium之前需要先<a target="_blank" rel="noopener" href="https://helm.sh/docs/intro/install/">安装helm3</a>。</p>
<p>helm3的部署非常的简单，我们只要去<a target="_blank" rel="noopener" href="https://github.com/helm/helm/releases">GitHub</a>找到对应系统版本的二进制文件，下载解压后放到系统的执行目录就可以使用了。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ wget https://get.helm.sh/helm-v3.10.2-linux-amd64.tar.gz<br>$ tar -zxvf helm-v3.10.2-linux-amd64.tar.gz<br>$ <span class="hljs-built_in">cp</span> -rp linux-amd64/helm /usr/local/bin/<br>$ helm version<br>version.BuildInfo&#123;Version:<span class="hljs-string">&quot;v3.10.2&quot;</span>, GitCommit:<span class="hljs-string">&quot;50f003e5ee8704ec937a756c646870227d7c8b58&quot;</span>, GitTreeState:<span class="hljs-string">&quot;clean&quot;</span>, GoVersion:<span class="hljs-string">&quot;go1.18.8&quot;</span>&#125;<br></code></pre></div></td></tr></table></figure>
<h2 id="5-2-部署cilium">5.2 部署cilium</h2>
<p>完整的部署指南可以<a target="_blank" rel="noopener" href="https://docs.cilium.io/en/stable/gettingstarted/k8s-install-helm/">参考官方文档</a>，首先我们添加helm的repo。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ helm repo add cilium https://helm.cilium.io/<br><span class="hljs-string">&quot;cilium&quot;</span> has been added to your repositories<br>$ helm repo list<br>NAME    URL<br>cilium  https://helm.cilium.io/<br></code></pre></div></td></tr></table></figure>
<p>参考官网的文档，这里我们需要指定集群的APIserver的IP和端口</p>
<figure class="highlight routeros"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs routeros">helm install cilium ./cilium \<br>    --namespace kube-system \<br>    --<span class="hljs-built_in">set</span> <span class="hljs-attribute">kubeProxyReplacement</span>=strict \<br>    --<span class="hljs-built_in">set</span> <span class="hljs-attribute">k8sServiceHost</span>=REPLACE_WITH_API_SERVER_IP \<br>    --<span class="hljs-built_in">set</span> <span class="hljs-attribute">k8sServicePort</span>=REPLACE_WITH_API_SERVER_PORT<br></code></pre></div></td></tr></table></figure>
<p>但是考虑到cilium默认使用的<code>podCIDR</code>为<code>10.0.0.0/8</code>，很可能会和我们集群内的网络冲突，最好的方案就是初始化的时候指定<code>podCIDR</code>，关于初始化的时候podCIDR的设置，可以参考官方的这个<a target="_blank" rel="noopener" href="https://docs.cilium.io/en/stable/gettingstarted/ipam-cluster-pool/">文章</a>。</p>
<figure class="highlight routeros"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs routeros">helm install cilium cilium/cilium --version 1.12.4 \<br>    --namespace kube-system \<br>    --<span class="hljs-built_in">set</span> <span class="hljs-attribute">k8sServiceHost</span>=REPLACE_WITH_API_SERVER_IP \<br>    --<span class="hljs-built_in">set</span> <span class="hljs-attribute">k8sServicePort</span>=REPLACE_WITH_API_SERVER_PORT \<br>	--<span class="hljs-built_in">set</span> ipam.operator.<span class="hljs-attribute">clusterPoolIPv4PodCIDRList</span>=&lt;IPv4CIDR&gt; \<br>	--<span class="hljs-built_in">set</span> ipam.operator.<span class="hljs-attribute">clusterPoolIPv4MaskSize</span>=&lt;IPv4MaskSize&gt;<br></code></pre></div></td></tr></table></figure>
<p>最后可以得到我们的初始化安装参数</p>
<figure class="highlight routeros"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs routeros">helm install cilium cilium/cilium --version 1.12.4 \<br>	--namespace kube-system \<br>	--<span class="hljs-built_in">set</span> <span class="hljs-attribute">k8sServiceHost</span>=k8s-cilium-apiserver.tinychen.io \<br>	--<span class="hljs-built_in">set</span> <span class="hljs-attribute">k8sServicePort</span>=8443 \<br>	--<span class="hljs-built_in">set</span> ipam.operator.<span class="hljs-attribute">clusterPoolIPv4PodCIDRList</span>=10.32.0.0/17 \<br>	--<span class="hljs-built_in">set</span> ipam.operator.<span class="hljs-attribute">clusterPoolIPv4MaskSize</span>=24<br></code></pre></div></td></tr></table></figure>
<p>然后我们使用指令进行安装</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ helm install cilium cilium/cilium --version 1.12.4 \<br>&gt; --namespace kube-system \<br>&gt; --<span class="hljs-built_in">set</span> k8sServiceHost=k8s-cilium-apiserver.tinychen.io \<br>&gt; --<span class="hljs-built_in">set</span> k8sServicePort=8443 \<br>&gt; --<span class="hljs-built_in">set</span> ipam.operator.clusterPoolIPv4PodCIDRList=10.32.0.0/17 \<br>&gt; --<span class="hljs-built_in">set</span> ipam.operator.clusterPoolIPv4MaskSize=24<br>NAME: cilium<br>LAST DEPLOYED: Thu Dec  8 17:01:13 2022<br>NAMESPACE: kube-system<br>STATUS: deployed<br>REVISION: 1<br>TEST SUITE: None<br>NOTES:<br>You have successfully installed Cilium with Hubble.<br><br>Your release version is 1.12.4.<br><br>For any further <span class="hljs-built_in">help</span>, visit https://docs.cilium.io/en/v1.12/gettinghelp<br></code></pre></div></td></tr></table></figure>
<p>此时我们再查看集群的daemonset和deployment状态：</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 这时候查看集群的daemonset和deployment状态可以看到cilium相关的服务已经正常</span><br>$ kubectl get ds -A<br>NAMESPACE     NAME         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE<br>kube-system   cilium       6         6         6       6            6           kubernetes.io/os=linux   3m14s<br>kube-system   kube-proxy   6         6         6       6            6           kubernetes.io/os=linux   49m<br><br>$ kubectl get deploy -A<br>NAMESPACE     NAME              READY   UP-TO-DATE   AVAILABLE   AGE<br>kube-system   cilium-operator   2/2     2            2           3m29s<br>kube-system   coredns           2/2     2            2           50m<br></code></pre></div></td></tr></table></figure>
<p>再查看所有的pod，状态都正常，ip也和我们初始化的时候分配的ip段一致，说明初始化的参数设置生效了。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 再查看所有的pod，状态都正常，ip按预期进行了分配</span><br><span class="hljs-variable">$#</span> kubectl get pods -A -o wide<br>NAMESPACE     NAME                                                               READY   STATUS    RESTARTS       AGE    IP            NODE                                       NOMINATED NODE   READINESS GATES<br>kube-system   cilium-5ppb6                                                       1/1     Running   0              3m4s   10.31.80.1    k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   cilium-chrch                                                       1/1     Running   0              3m4s   10.31.80.5    k8s-cilium-worker-10-31-80-5.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   cilium-f2sdc                                                       1/1     Running   0              3m4s   10.31.80.4    k8s-cilium-worker-10-31-80-4.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   cilium-fbrdl                                                       1/1     Running   0              3m4s   10.31.80.6    k8s-cilium-worker-10-31-80-6.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   cilium-g7dzj                                                       1/1     Running   0              3m4s   10.31.80.3    k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   cilium-g7k5m                                                       1/1     Running   0              3m4s   10.31.80.2    k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   cilium-operator-5598954549-62q7t                                   1/1     Running   0              3m4s   10.31.80.4    k8s-cilium-worker-10-31-80-4.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   cilium-operator-5598954549-zctb8                                   1/1     Running   0              3m4s   10.31.80.6    k8s-cilium-worker-10-31-80-6.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   coredns-c676cc86f-6jg9b                                            1/1     Running   0              49m    10.32.0.64    k8s-cilium-worker-10-31-80-6.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   coredns-c676cc86f-qmx9s                                            1/1     Running   0              49m    10.32.0.145   k8s-cilium-worker-10-31-80-6.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   etcd-k8s-cilium-master-10-31-80-1.tinychen.io                      1/1     Running   0              49m    10.31.80.1    k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   etcd-k8s-cilium-master-10-31-80-2.tinychen.io                      1/1     Running   0              45m    10.31.80.2    k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   etcd-k8s-cilium-master-10-31-80-3.tinychen.io                      1/1     Running   0              40m    10.31.80.3    k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   haproxy-k8s-cilium-master-10-31-80-1.tinychen.io                   1/1     Running   12 (28m ago)   49m    10.31.80.1    k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   haproxy-k8s-cilium-master-10-31-80-2.tinychen.io                   1/1     Running   9 (29m ago)    45m    10.31.80.2    k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   haproxy-k8s-cilium-master-10-31-80-3.tinychen.io                   1/1     Running   8 (29m ago)    40m    10.31.80.3    k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   keepalived-k8s-cilium-master-10-31-80-1.tinychen.io                1/1     Running   0              49m    10.31.80.1    k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   keepalived-k8s-cilium-master-10-31-80-2.tinychen.io                1/1     Running   0              45m    10.31.80.2    k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   keepalived-k8s-cilium-master-10-31-80-3.tinychen.io                1/1     Running   0              40m    10.31.80.3    k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-apiserver-k8s-cilium-master-10-31-80-1.tinychen.io            1/1     Running   0              49m    10.31.80.1    k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-apiserver-k8s-cilium-master-10-31-80-2.tinychen.io            1/1     Running   0              45m    10.31.80.2    k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-apiserver-k8s-cilium-master-10-31-80-3.tinychen.io            1/1     Running   0              40m    10.31.80.3    k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-controller-manager-k8s-cilium-master-10-31-80-1.tinychen.io   1/1     Running   1 (45m ago)    49m    10.31.80.1    k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-controller-manager-k8s-cilium-master-10-31-80-2.tinychen.io   1/1     Running   0              45m    10.31.80.2    k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-controller-manager-k8s-cilium-master-10-31-80-3.tinychen.io   1/1     Running   0              40m    10.31.80.3    k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-52zmk                                                   1/1     Running   0              49m    10.31.80.1    k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-dg4nt                                                   1/1     Running   0              37m    10.31.80.5    k8s-cilium-worker-10-31-80-5.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-gbgr5                                                   1/1     Running   0              45m    10.31.80.2    k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-gpzxv                                                   1/1     Running   0              37m    10.31.80.6    k8s-cilium-worker-10-31-80-6.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-kn9gq                                                   1/1     Running   0              40m    10.31.80.3    k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-proxy-xw8nl                                                   1/1     Running   0              38m    10.31.80.4    k8s-cilium-worker-10-31-80-4.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-scheduler-k8s-cilium-master-10-31-80-1.tinychen.io            1/1     Running   1 (45m ago)    49m    10.31.80.1    k8s-cilium-master-10-31-80-1.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-scheduler-k8s-cilium-master-10-31-80-2.tinychen.io            1/1     Running   0              45m    10.31.80.2    k8s-cilium-master-10-31-80-2.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>kube-system   kube-scheduler-k8s-cilium-master-10-31-80-3.tinychen.io            1/1     Running   0              40m    10.31.80.3    k8s-cilium-master-10-31-80-3.tinychen.io   &lt;none&gt;           &lt;none&gt;<br></code></pre></div></td></tr></table></figure>
<p>这时候我们再进入pod中检查cilium的状态</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># --verbose参数可以查看详细的状态信息</span><br><span class="hljs-comment"># cilium-97fn7需要替换为任意一个cilium的pod</span><br>$ kubectl <span class="hljs-built_in">exec</span> -it -n kube-system cilium-5ppb6 -- cilium status --verbose<br>Defaulted container <span class="hljs-string">&quot;cilium-agent&quot;</span> out of: cilium-agent, mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init)<br>KVStore:                Ok   Disabled<br>Kubernetes:             Ok   1.25 (v1.25.4) [linux/amd64]<br>Kubernetes APIs:        [<span class="hljs-string">&quot;cilium/v2::CiliumClusterwideNetworkPolicy&quot;</span>, <span class="hljs-string">&quot;cilium/v2::CiliumEndpoint&quot;</span>, <span class="hljs-string">&quot;cilium/v2::CiliumNetworkPolicy&quot;</span>, <span class="hljs-string">&quot;cilium/v2::CiliumNode&quot;</span>, <span class="hljs-string">&quot;core/v1::Namespace&quot;</span>, <span class="hljs-string">&quot;core/v1::Node&quot;</span>, <span class="hljs-string">&quot;core/v1::Pods&quot;</span>, <span class="hljs-string">&quot;core/v1::Service&quot;</span>, <span class="hljs-string">&quot;discovery/v1::EndpointSlice&quot;</span>, <span class="hljs-string">&quot;networking.k8s.io/v1::NetworkPolicy&quot;</span>]<br>KubeProxyReplacement:   Disabled<br>Host firewall:          Disabled<br>CNI Chaining:           none<br>Cilium:                 Ok   1.12.4 (v1.12.4-6eaecaf)<br>NodeMonitor:            Listening <span class="hljs-keyword">for</span> events on 16 CPUs with 64x4096 of shared memory<br>Cilium health daemon:   Ok<br>IPAM:                   IPv4: 2/254 allocated from 10.32.3.0/24,<br>Allocated addresses:<br>  10.32.3.145 (router)<br>  10.32.3.170 (health)<br>BandwidthManager:       Disabled<br>Host Routing:           Legacy<br>Masquerading:           IPTables [IPv4: Enabled, IPv6: Disabled]<br>Clock Source <span class="hljs-keyword">for</span> BPF:   ktime<br>Controller Status:      18/18 healthy<br>  Name                                  Last success   Last error   Count   Message<br>  cilium-health-ep                      10s ago        never        0       no error<br>  dns-garbage-collector-job             16s ago        never        0       no error<br>  endpoint-1050-regeneration-recovery   never          never        0       no error<br>  endpoint-671-regeneration-recovery    never          never        0       no error<br>  endpoint-gc                           2m17s ago      never        0       no error<br>  ipcache-inject-labels                 2m13s ago      2m15s ago    0       no error<br>  k8s-heartbeat                         17s ago        never        0       no error<br>  link-cache                            11s ago        never        0       no error<br>  metricsmap-bpf-prom-sync              1s ago         never        0       no error<br>  resolve-identity-1050                 2m10s ago      never        0       no error<br>  resolve-identity-671                  2m11s ago      never        0       no error<br>  sync-endpoints-and-host-ips           11s ago        never        0       no error<br>  sync-lb-maps-with-k8s-services        2m11s ago      never        0       no error<br>  sync-policymap-1050                   1m1s ago       never        0       no error<br>  sync-policymap-671                    9s ago         never        0       no error<br>  sync-to-k8s-ciliumendpoint (1050)     10s ago        never        0       no error<br>  sync-to-k8s-ciliumendpoint (671)      1s ago         never        0       no error<br>  template-dir-watcher                  never          never        0       no error<br>Proxy Status:            OK, ip 10.32.3.145, 0 redirects active on ports 10000-20000<br>Global Identity Range:   min 256, max 65535<br>Hubble:                  Ok   Current/Max Flows: 250/4095 (6.11%), Flows/s: 1.74   Metrics: Disabled<br>KubeProxyReplacement Details:<br>  Status:                 Disabled<br>  Socket LB:              Disabled<br>  Session Affinity:       Disabled<br>  Graceful Termination:   Enabled<br>  NAT46/64 Support:       Disabled<br>  Services:<br>  - ClusterIP:      Enabled<br>  - NodePort:       Disabled<br>  - LoadBalancer:   Disabled<br>  - externalIPs:    Disabled<br>  - HostPort:       Disabled<br>BPF Maps:   dynamic sizing: on (ratio: 0.002500)<br>  Name                          Size<br>  Non-TCP connection tracking   73621<br>  TCP connection tracking       147243<br>  Endpoint policy               65535<br>  Events                        16<br>  IP cache                      512000<br>  IP masquerading agent         16384<br>  IPv4 fragmentation            8192<br>  IPv4 service                  65536<br>  IPv6 service                  65536<br>  IPv4 service backend          65536<br>  IPv6 service backend          65536<br>  IPv4 service reverse NAT      65536<br>  IPv6 service reverse NAT      65536<br>  Metrics                       1024<br>  NAT                           147243<br>  Neighbor table                147243<br>  Global policy                 16384<br>  Per endpoint policy           65536<br>  Session affinity              65536<br>  Signal                        16<br>  Sockmap                       65535<br>  Sock reverse NAT              73621<br>  Tunnel                        65536<br>Encryption:                                              Disabled<br>Cluster health:                                          6/6 reachable   (2022-12-08T09:03:25Z)<br>  Name                                                   IP              Node        Endpoints<br>  k8s-cilium-master-10-31-80-1.tinychen.io (localhost)   10.31.80.1      reachable   reachable<br>  k8s-cilium-master-10-31-80-2.tinychen.io               10.31.80.2      reachable   reachable<br>  k8s-cilium-master-10-31-80-3.tinychen.io               10.31.80.3      reachable   reachable<br>  k8s-cilium-worker-10-31-80-4.tinychen.io               10.31.80.4      reachable   reachable<br>  k8s-cilium-worker-10-31-80-5.tinychen.io               10.31.80.5      reachable   reachable<br>  k8s-cilium-worker-10-31-80-6.tinychen.io               10.31.80.6      reachable   reachable<br></code></pre></div></td></tr></table></figure>
<p>其实到这里cilium的部署就可以说是ok了的，整个集群的cni都处于正常状态，其余的工作负载也都能够正常运行了。</p>
<h2 id="5-3-部署hubble">5.3 部署hubble</h2>
<p>cilium还有一大特点就是其可观测性比其他的cni要优秀很多，想要体验到cilium的可观测性，我们就需要在k8s集群中<a target="_blank" rel="noopener" href="https://docs.cilium.io/en/stable/gettingstarted/hubble_setup/#hubble-setup">安装hubble</a>。同时hubble提供了ui界面来更好的实现集群内网络的可观测性，这里我们也一并把<code>hubble-ui</code>安装上。</p>
<h3 id="helm3安装hubble">helm3安装hubble</h3>
<p>我们继续接着上面的helm3来安装hubble，因为我们已经安装了cilium，因此这里需要使用<code>upgrade</code>来进行升级安装，并且使用<code>--reuse-values</code>来复用之前的安装参数</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">helm upgrade cilium cilium/cilium --version 1.12.4 \<br>   --namespace kube-system \<br>   --reuse-values \<br>   --<span class="hljs-built_in">set</span> hubble.relay.enabled=<span class="hljs-literal">true</span> \<br>   --<span class="hljs-built_in">set</span> hubble.ui.enabled=<span class="hljs-literal">true</span><br></code></pre></div></td></tr></table></figure>
<p>然后我们直接进行安装</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ helm upgrade cilium cilium/cilium --version 1.12.4 \<br>&gt;    --namespace kube-system \<br>&gt;    --reuse-values \<br>&gt;    --<span class="hljs-built_in">set</span> hubble.relay.enabled=<span class="hljs-literal">true</span> \<br>&gt;    --<span class="hljs-built_in">set</span> hubble.ui.enabled=<span class="hljs-literal">true</span><br>Release <span class="hljs-string">&quot;cilium&quot;</span> has been upgraded. Happy Helming!<br>NAME: cilium<br>LAST DEPLOYED: Thu Dec  8 17:06:51 2022<br>NAMESPACE: kube-system<br>STATUS: deployed<br>REVISION: 2<br>TEST SUITE: None<br>NOTES:<br>You have successfully installed Cilium with Hubble Relay and Hubble UI.<br><br>Your release version is 1.12.4.<br><br>For any further <span class="hljs-built_in">help</span>, visit https://docs.cilium.io/en/v1.12/gettinghelp<br></code></pre></div></td></tr></table></figure>
<p>随后我们查看相关的集群状态，可以看到相对应的pod、deploy和svc都工作正常</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl get pod -A | grep hubble<br>kube-system   hubble-relay-67ffc5f588-qr8nt                                      1/1     Running   0              56s<br>kube-system   hubble-ui-5dc4d884b6-84qlp                                         2/2     Running   0              56s<br>$ kubectl get deploy -A | grep hubble<br>kube-system   hubble-relay      1/1     1            1           3m12s<br>kube-system   hubble-ui         1/1     1            1           3m12s<br>$ kubectl get svc -A | grep hubble<br>kube-system   hubble-peer    ClusterIP   10.32.131.127   &lt;none&gt;        443/TCP                  9m3s<br>kube-system   hubble-relay   ClusterIP   10.32.171.0     &lt;none&gt;        80/TCP                   3m22s<br>kube-system   hubble-ui      ClusterIP   10.32.184.206   &lt;none&gt;        80/TCP                   3m22s<br></code></pre></div></td></tr></table></figure>
<h3 id="cilium-cli安装hubble">cilium-cli安装hubble</h3>
<p>使用cilium-cli功能来安装hubble也非常简单：</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 首先安装cilium-cli工具</span><br><span class="hljs-comment"># cilium的cli工具是一个二进制的可执行文件</span><br>$ curl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz&#123;,.<span class="hljs-built_in">sha256sum</span>&#125;<br>$ <span class="hljs-built_in">sha256sum</span> --check cilium-linux-amd64.tar.gz.sha256sum<br>cilium-linux-amd64.tar.gz: OK<br>$ sudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin<br>cilium<br><br><span class="hljs-comment"># 然后直接启用hubble</span><br>$ cilium hubble <span class="hljs-built_in">enable</span><br><span class="hljs-comment"># 再启用hubble-ui</span><br>$ cilium hubble <span class="hljs-built_in">enable</span> --ui<br><span class="hljs-comment"># 接着查看cilium状态</span><br>$ cilium status<br>    /¯¯\<br> /¯¯\__/¯¯\    Cilium:         OK<br> \__/¯¯\__/    Operator:       OK<br> /¯¯\__/¯¯\    Hubble:         OK<br> \__/¯¯\__/    ClusterMesh:    disabled<br>    \__/<br><br>Deployment        hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1<br>DaemonSet         cilium             Desired: 6, Ready: 6/6, Available: 6/6<br>Deployment        hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1<br>Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2<br>Containers:       cilium             Running: 6<br>                  hubble-relay       Running: 1<br>                  cilium-operator    Running: 2<br>                  hubble-ui          Running: 1<br>Cluster Pods:     4/4 managed by Cilium<br>Image versions    cilium-operator    quay.io/cilium/operator-generic:v1.12.4@sha256:071089ec5bca1f556afb8e541d9972a0dfb09d1e25504ae642ced021ecbedbd1: 2<br>                  hubble-ui          quay.io/cilium/hubble-ui-backend:v0.9.2@sha256:a3ac4d5b87889c9f7cc6323e86d3126b0d382933bd64f44382a92778b0cde5d7: 1<br>                  hubble-ui          quay.io/cilium/hubble-ui:v0.9.2@sha256:d3596efc94a41c6b772b9afe6fe47c17417658956e04c3e2a28d293f2670663e: 1<br>                  cilium             quay.io/cilium/cilium:v1.12.4@sha256:4b074fcfba9325c18e97569ed1988464309a5ebf64bbc79bec6f3d58cafcb8cf: 6<br>                  hubble-relay       quay.io/cilium/hubble-relay:v1.12.4@sha256:dc5b396e94f986f83ccef89f13a91c29df482d4af491ff3bd4d40c05873d351a: 1<br></code></pre></div></td></tr></table></figure>
<h3 id="安装hubble客户端">安装hubble客户端</h3>
<p>和cilium一样，hubble也提供了一个客户端来让我们操作</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 首先我们需要安装hubble的客户端，安装原理和过程与安装cilium几乎一致</span><br>$ <span class="hljs-built_in">export</span> HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)<br>$ curl -L --remote-name-all https://github.com/cilium/hubble/releases/download/<span class="hljs-variable">$HUBBLE_VERSION</span>/hubble-linux-amd64.tar.gz&#123;,.<span class="hljs-built_in">sha256sum</span>&#125;<br>$ <span class="hljs-built_in">sha256sum</span> --check hubble-linux-amd64.tar.gz.sha256sum<br>$ sudo tar xzvfC hubble-linux-amd64.tar.gz /usr/local/bin<br>$ <span class="hljs-built_in">rm</span> hubble-linux-amd64.tar.gz&#123;,.<span class="hljs-built_in">sha256sum</span>&#125;<br>$ hubble --version<br>hubble v0.10.0<br></code></pre></div></td></tr></table></figure>
<p>然后我们需要暴露hubble api服务的端口，直接使用kubectl的port-forward功能把hubble-relay这个服务的80端口暴露到4245端口上</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 仅暴露在IPV4网络中</span><br>$ kubectl port-forward -n kube-system svc/hubble-relay --address 0.0.0.0 4245:80 &amp;<br><span class="hljs-comment"># 同时暴露在IPV6和IPV4网络中</span><br>$ kubectl port-forward -n kube-system svc/hubble-relay --address 0.0.0.0 --address :: 4245:80 &amp;<br></code></pre></div></td></tr></table></figure>
<p>如果使用cilium-cli工具安装的hubble也可以使用cilium暴露api端口，需要注意的是该命令默认会暴露到IPV6和IPV4网络中，如果宿主机节点不支持ipv6网络会报错</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ cilium hubble port-forward&amp;<br></code></pre></div></td></tr></table></figure>
<p>api端口暴露完成之后我们就可以测试一下hubble客户端的工作状态是否正常</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-string">$</span> <span class="hljs-string">hubble</span> <span class="hljs-string">status</span><br><span class="hljs-string">Handling</span> <span class="hljs-string">connection</span> <span class="hljs-string">for</span> <span class="hljs-number">4245</span><br><span class="hljs-string">Healthcheck</span> <span class="hljs-string">(via</span> <span class="hljs-string">localhost:4245):</span> <span class="hljs-string">Ok</span><br><span class="hljs-attr">Current/Max Flows:</span> <span class="hljs-number">10</span><span class="hljs-string">,903/12,285</span> <span class="hljs-string">(88.75%)</span><br><span class="hljs-attr">Flows/s:</span> <span class="hljs-number">5.98</span><br><span class="hljs-attr">Connected Nodes:</span> <span class="hljs-number">3</span><span class="hljs-string">/3</span><br></code></pre></div></td></tr></table></figure>
<p>这里需要注意如果发现hubble的状态不正常，查看日志发现</p>
<figure class="highlight routeros"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs routeros">$ kubectl logs -f  hubble-relay-67ffc5f588-qr8nt -n kube-system<br> <span class="hljs-attribute">level</span>=warning <span class="hljs-attribute">msg</span>=<span class="hljs-string">&quot;Failed to create peer client for peers synchronization; will try again after the timeout has expired&quot;</span> <span class="hljs-attribute">error</span>=<span class="hljs-string">&quot;context deadline exceeded&quot;</span> <span class="hljs-attribute">subsys</span>=hubble-relay <span class="hljs-attribute">target</span>=<span class="hljs-string">&quot;hubble-peer.kube-system.svc.cluster.local:443&quot;</span><br></code></pre></div></td></tr></table></figure>
<p>主要是因为前面初始化的时候我们定义了集群名为<code>cili-cluster.tclocal</code>，因此集群中coredns的配置没有<code>cluster.local</code>的解析，我们手动增加一个即可。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl get cm coredns -n kube-system -o yaml<br>apiVersion: v1<br>data:<br>  Corefile: |<br>    .:53 &#123;<br>        errors<br>        health &#123;<br>           lameduck 5s<br>        &#125;<br>        ready<br>        kubernetes cili-cluster.tclocal in-addr.arpa ip6.arpa &#123;<br>           pods insecure<br>           fallthrough in-addr.arpa ip6.arpa<br>           ttl 30<br>        &#125;<br>        prometheus :9153<br>        forward . /etc/resolv.conf &#123;<br>           max_concurrent 1000<br>        &#125;<br>        cache 30<br>        loop<br>        reload<br>        loadbalance<br>    &#125;<br>    cluster.local.:53 &#123;<br>        errors<br>        health &#123;<br>           lameduck 5s<br>        &#125;<br>        ready<br>        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;<br>           pods insecure<br>           fallthrough in-addr.arpa ip6.arpa<br>           ttl 30<br>        &#125;<br>        prometheus :9153<br>        cache 30<br>        loop<br>        reload<br>        loadbalance<br>    &#125;<br>kind: ConfigMap<br>metadata:<br>  creationTimestamp: <span class="hljs-string">&quot;2022-12-08T08:14:37Z&quot;</span><br>  name: coredns<br>  namespace: kube-system<br>  resourceVersion: <span class="hljs-string">&quot;11835&quot;</span><br>  uid: fbdcef03-87a6-4ddb-b620-c55a17c0d7d7<br></code></pre></div></td></tr></table></figure>
<h3 id="暴露hubble-ui">暴露hubble-ui</h3>
<p>官方介绍里面是使用cilium工具直接暴露hubble-ui的访问端口到宿主机上面的12000端口</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 将hubble-ui这个服务的80端口暴露到宿主机上面的12000端口上面</span><br>$ cilium hubble ui&amp;<br>[2] 5809<br>ℹ️  Opening <span class="hljs-string">&quot;http://localhost:12000&quot;</span> <span class="hljs-keyword">in</span> your browser...<br><br></code></pre></div></td></tr></table></figure>
<p>实际上执行的操作等同于下面这个命令</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 同时暴露在IPV6和IPV4网络中</span><br><span class="hljs-comment"># kubectl port-forward -n kube-system svc/hubble-ui --address 0.0.0.0 --address :: 12000:80</span><br><br><span class="hljs-comment"># 仅暴露在IPV4网络中</span><br><span class="hljs-comment"># kubectl port-forward -n kube-system svc/hubble-ui --address 0.0.0.0 12000:80</span><br></code></pre></div></td></tr></table></figure>
<p>这里我们使用nodeport的方式来暴露hubble-ui，首先我们查看原来的<code>hubble-ui</code>这个svc的配置</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl get svc -n kube-system hubble-ui -o yaml<br>...此处略去一堆输出...<br>  - name: http<br>    port: 80<br>    protocol: TCP<br>    targetPort: 8081<br>  selector:<br>    k8s-app: hubble-ui<br>  sessionAffinity: None<br>  <span class="hljs-built_in">type</span>: ClusterIP<br>...此处略去一堆输出...<br></code></pre></div></td></tr></table></figure>
<p>我们把默认的ClusterIP修改为<code>NodePort</code>，并且指定端口为<code>nodePort: 30081</code></p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl get svc -n kube-system hubble-ui -o yaml<br>...此处略去一堆输出...<br>  ports:<br>  - name: http<br>    nodePort: 30081<br>    port: 80<br>    protocol: TCP<br>    targetPort: 8081<br>  selector:<br>    k8s-app: hubble-ui<br>  sessionAffinity: None<br>  <span class="hljs-built_in">type</span>: NodePort<br>...此处略去一堆输出...<br></code></pre></div></td></tr></table></figure>
<p>修改前后对比查看状态</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 修改前，使用ClusterIP</span><br>$ kubectl get svc -A | grep hubble-ui<br>kube-system   hubble-ui      ClusterIP   10.32.184.206   &lt;none&gt;        80/TCP                   82s<br><br><span class="hljs-comment"># 修改后，使用NodePort</span><br>$ kubectl get svc -A | grep hubble-ui<br>kube-system   hubble-ui      NodePort    10.32.184.206   &lt;none&gt;        80:30081/TCP             13m<br></code></pre></div></td></tr></table></figure>
<p>这时候我们在浏览器中访问<code>http://10.31.80.1:30081</code>就可以看到hubble的ui界面了</p>
<p><img src="https://resource.tinychen.com/202212081751249.png" srcset="/img/loading.gif" lazyload alt=""></p>
<h1>6、配置BGP路由</h1>
<h2 id="6-1-使用bird2">6.1 使用bird2</h2>
<p>cilium官方给出的基于bird宣告BGP路由的架构图如下：</p>
<p><img src="https://resource.tinychen.com/202212091530104.png" srcset="/img/loading.gif" lazyload alt=""></p>
<ul>
<li>这里假设的是存在两个核心路由器，实际上根据网络环境的不同可以动态变化（可以是多个路由器或者是能跑BGP路由的三层交换机），在下面的测试环境中我们只使用一个openwrt来充当路由器的角色</li>
<li><code>bird</code>不从核心路由器和其他节点学习路由，这使得每个节点的内核路由表保持干净和小，并且没有性能问题（这里的性能问题指的是bird的性能问题）。</li>
<li>在这个方案中，每个节点只是将 pod 出口流量发送到节点的默认网关（核心路由器），并让后者进行路由。</li>
</ul>
<p><strong>上述的这个方案最大的特点就是每个node节点都会把整个pod的CIDR发布到对端路由器上，并不会精确控制每个node的路由条目。</strong></p>
<p>好处是只要还有一个node的BGP连接正常，那么集群外部就能够访问所有node节点上面的pod；坏处就是集群外部访问pod的流量不一定会直接转发到对应pod所在的node上面，很可能会转发到其他的node上面，再通过这个node上面的具体路由走隧道转发到对应的pod上。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 首先使用yum安装bird2</span><br>$ yum install bird2 -y<br><br><span class="hljs-comment"># 如果没有bird2的话可以先添加epel源</span><br>$ yum install epel-release -y<br><br><span class="hljs-comment"># 配置好开机启动</span><br>$ systemctl <span class="hljs-built_in">enable</span> bird<br>$ systemctl restart bird<br><br>$ birdc show route<br>BIRD 2.0.10 ready.<br></code></pre></div></td></tr></table></figure>
<p>bird的配置文件也是相对比较简单，下面是其中一台机器的配置，这里10.31.80.1是本机IP，64515是cilium集群的ASN号，而10.31.254.253是路由器的IP，64512是路由器端的ASN号。</p>
<p>注意这里的配置开启了BFD、ECMP和graceful restart，更详细的高级配置可以参考<a target="_blank" rel="noopener" href="https://docs.cilium.io/en/stable/gettingstarted/bird/#advanced-configurations">cilium的官方文档</a>。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-built_in">log</span> syslog all;<br><br>router <span class="hljs-built_in">id</span> 10.31.80.1;<br><br>protocol device &#123;<br>        scan time 10;           <span class="hljs-comment"># Scan interfaces every 10 seconds</span><br>&#125;<br><br><span class="hljs-comment"># Disable automatically generating direct routes to all network interfaces.</span><br>protocol direct &#123;<br>        disabled;               <span class="hljs-comment"># Disable by default</span><br>&#125;<br><br><span class="hljs-comment"># Forbid synchronizing BIRD routing tables with the OS kernel.</span><br>protocol kernel &#123;<br>        ipv4 &#123;                    <span class="hljs-comment"># Connect protocol to IPv4 table by channel</span><br>                import none;      <span class="hljs-comment"># Import to table, default is import all</span><br>                <span class="hljs-built_in">export</span> none;      <span class="hljs-comment"># Export to protocol. default is export none</span><br>        &#125;;<br><br>        <span class="hljs-comment"># Configure ECMP</span><br>        merge paths <span class="hljs-built_in">yes</span> <span class="hljs-built_in">limit</span> 16 ;<br>&#125;<br><br><span class="hljs-comment"># Static IPv4 routes.</span><br>protocol static &#123;<br>      ipv4;<br>      route 10.32.0.0/17 via <span class="hljs-string">&quot;cilium_host&quot;</span>;<br>&#125;<br><br>protocol bfd &#123;<br>      interface <span class="hljs-string">&quot;&#123;&#123; grains[&#x27;node_mgnt_device&#x27;] &#125;&#125;&quot;</span> &#123;<br>              min rx interval 100 ms;<br>              min tx interval 100 ms;<br>              idle tx interval 300 ms;<br>              multiplier 10;<br>      &#125;;<br><br>      neighbor 10.31.254.253;<br>&#125;<br><br><span class="hljs-comment"># BGP peers</span><br>protocol bgp uplink0 &#123;<br>      description <span class="hljs-string">&quot;OpenWRT BGP uplink 0&quot;</span>;<br>      <span class="hljs-built_in">local</span> 10.31.80.1 as 64515;<br>      neighbor 10.31.254.253 as 64512;<br>      graceful restart;<br>      bfd on;<br><br>      ipv4 &#123;<br>              import filter &#123;reject;&#125;;<br>              <span class="hljs-built_in">export</span> filter &#123;accept;&#125;;<br>      &#125;;<br>&#125;<br></code></pre></div></td></tr></table></figure>
<p>六个机器都成功配置之后我们在路由器侧应该是可以看到类似的路由条目，此时在集群外的机器上面<strong>如果添加了对应pod网段的路由</strong>的话，是可以通过pod IP直接访问到集群内的pod。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 查看路由器上面的状态</span><br>tiny-openwrt<span class="hljs-comment"># show ip route</span><br>......<br>B&gt;* 10.32.0.0/17 [20/0] via 10.31.80.1, eth0, weight 1, 00:00:12<br>  *                     via 10.31.80.2, eth0, weight 1, 00:00:12<br>  *                     via 10.31.80.3, eth0, weight 1, 00:00:12<br>  *                     via 10.31.80.4, eth0, weight 1, 00:00:12<br>  *                     via 10.31.80.5, eth0, weight 1, 00:00:12<br>  *                     via 10.31.80.6, eth0, weight 1, 00:00:12<br>......<br><br><span class="hljs-comment"># 查看对应node上面的bird状态</span><br>[root@k8s-cilium-master-10-31-80-1 ~]<span class="hljs-comment"># birdc</span><br>BIRD 2.0.10 ready.<br>bird&gt; show route<br>Table master4:<br>10.32.0.0/17         unicast [static1 15:33:26.703] * (200)<br>        dev cilium_host<br>bird&gt;<br></code></pre></div></td></tr></table></figure>
<h2 id="6-2-使用kube-router">6.2 使用kube-router</h2>
<h3 id="6-2-1-配置kube-router">6.2.1 配置kube-router</h3>
<figure class="highlight awk"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs awk">curl -LO https:<span class="hljs-regexp">//</span>raw.githubusercontent.com<span class="hljs-regexp">/cloudnativelabs/</span>kube-router<span class="hljs-regexp">/v1.2/</span>daemonset/generic-kuberouter-only-advertise-routes.yaml<br></code></pre></div></td></tr></table></figure>
<p>默认的arg参数如下，完整的配置参考<a target="_blank" rel="noopener" href="https://github.com/cloudnativelabs/kube-router/blob/master/docs/user-guide.md">官方文档</a></p>
<figure class="highlight avrasm"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs avrasm"><span class="hljs-symbol">args:</span><br>- <span class="hljs-string">&quot;--run-router=true&quot;</span><br>- <span class="hljs-string">&quot;--run-firewall=false&quot;</span><br>- <span class="hljs-string">&quot;--run-service-proxy=false&quot;</span><br>- <span class="hljs-string">&quot;--bgp-graceful-restart=true&quot;</span><br>- <span class="hljs-string">&quot;--enable-cni=false&quot;</span><br>- <span class="hljs-string">&quot;--enable-ibgp=false&quot;</span><br>- <span class="hljs-string">&quot;--enable-overlay=false&quot;</span><br>- <span class="hljs-string">&quot;--peer-router-ips=&lt;CHANGE ME&gt;&quot;</span><br>- <span class="hljs-string">&quot;--peer-router-asns=&lt;CHANGE ME&gt;&quot;</span><br>- <span class="hljs-string">&quot;--cluster-asn=&lt;CHANGE ME&gt;&quot;</span><br>- <span class="hljs-string">&quot;--advertise-cluster-ip=true&quot;</span><br>- <span class="hljs-string">&quot;--advertise-external-ip=true&quot;</span><br>- <span class="hljs-string">&quot;--advertise-loadbalancer-ip=true&quot;</span><br></code></pre></div></td></tr></table></figure>
<p>我们需要对其进行修改，官方表示下面的这些参数必须要和要求的一致：</p>
<figure class="highlight 1c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs 1c">- <span class="hljs-string">&quot;--run-router=true&quot;</span><br>- <span class="hljs-string">&quot;--run-firewall=false&quot;</span><br>- <span class="hljs-string">&quot;--run-service-proxy=false&quot;</span><br>- <span class="hljs-string">&quot;--enable-cni=false&quot;</span><br>- <span class="hljs-string">&quot;--enable-pod-egress=false&quot;</span><br></code></pre></div></td></tr></table></figure>
<p>这些参数建议一致：</p>
<figure class="highlight 1c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs 1c">- <span class="hljs-string">&quot;--enable-ibgp=true&quot;</span><br>- <span class="hljs-string">&quot;--enable-overlay=true&quot;</span><br>- <span class="hljs-string">&quot;--advertise-cluster-ip=true&quot;</span><br>- <span class="hljs-string">&quot;--advertise-external-ip=true&quot;</span><br>- <span class="hljs-string">&quot;--advertise-loadbalancer-ip=true&quot;</span><br></code></pre></div></td></tr></table></figure>
<p>最后剩下的参数就是根据实际的网络状态来进行配置</p>
<figure class="highlight 1c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs 1c">- <span class="hljs-string">&quot;--peer-router-ips=10.31.254.253&quot;</span><br>- <span class="hljs-string">&quot;--peer-router-asns=64512&quot;</span><br>- <span class="hljs-string">&quot;--cluster-asn=64515&quot;</span><br></code></pre></div></td></tr></table></figure>
<p>如果需要同时和多个BGP peer建立连接可以参考官方的这个配置</p>
<figure class="highlight 1c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs 1c">- <span class="hljs-string">&quot;--cluster-asn=65001&quot;</span><br>- <span class="hljs-string">&quot;--peer-router-ips=10.0.0.1,10.0.2&quot;</span><br>- <span class="hljs-string">&quot;--peer-router-asns=65000,65000&quot;</span><br></code></pre></div></td></tr></table></figure>
<p>最后我这里使用的参数如下</p>
<figure class="highlight 1c"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs 1c">- <span class="hljs-string">&quot;--run-router=true&quot;</span><br>- <span class="hljs-string">&quot;--run-firewall=false&quot;</span><br>- <span class="hljs-string">&quot;--run-service-proxy=false&quot;</span><br>- <span class="hljs-string">&quot;--enable-cni=false&quot;</span><br>- <span class="hljs-string">&quot;--enable-pod-egress=false&quot;</span><br><br>- <span class="hljs-string">&quot;--enable-ibgp=true&quot;</span><br>- <span class="hljs-string">&quot;--enable-overlay=true&quot;</span><br>- <span class="hljs-string">&quot;--advertise-cluster-ip=true&quot;</span><br>- <span class="hljs-string">&quot;--advertise-external-ip=true&quot;</span><br>- <span class="hljs-string">&quot;--advertise-loadbalancer-ip=true&quot;</span><br><br>- <span class="hljs-string">&quot;--bgp-graceful-restart=true&quot;</span><br>- <span class="hljs-string">&quot;--peer-router-ips=10.31.254.253&quot;</span><br>- <span class="hljs-string">&quot;--peer-router-asns=64512&quot;</span><br>- <span class="hljs-string">&quot;--cluster-asn=64515&quot;</span><br></code></pre></div></td></tr></table></figure>
<blockquote>
<p>注意在官方文档中说明了–advertise-pod-cidr这个参数才是真正的把pod对应的CIDR发布出去的，但是由于这个值默认是true，因此我们不需要在这里进行显性配置。</p>
<p>–advertise-pod-cidr               Add Node’s POD cidr to the RIB so that it gets advertised to the BGP peers. (default true)</p>
</blockquote>
<p>配置完成之后我们直接进行部署</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 直接使用kubectl 进行部署</span><br>$ kubectl apply -f generic-kuberouter-only-advertise-routes.yaml<br>daemonset.apps/kube-router created<br>serviceaccount/kube-router created<br>clusterrole.rbac.authorization.k8s.io/kube-router created<br>clusterrolebinding.rbac.authorization.k8s.io/kube-router created<br><br><span class="hljs-comment"># 最后我们检查一下pod的运行状态</span><br>$ kubectl -n kube-system get pods -l k8s-app=kube-router<br>NAME                READY   STATUS    RESTARTS   AGE<br>kube-router-498zv   1/1     Running   0          78s<br>kube-router-8jm6b   1/1     Running   0          78s<br>kube-router-cknvc   1/1     Running   0          78s<br>kube-router-hgglx   1/1     Running   0          78s<br>kube-router-p5tks   1/1     Running   0          78s<br>kube-router-rjdbh   1/1     Running   0          78s<br></code></pre></div></td></tr></table></figure>
<h3 id="6-2-2-配置cilium">6.2.2 配置cilium</h3>
<p>随后我们还需要修改cilium的配置：</p>
<ul>
<li>将<code>ipam</code>修改为<code>kubernetes</code>，因为<code>kube-router</code>是从k8s直接获取CIDR信息的</li>
<li>将<code>tunnel</code>修改为<code>disabled</code>，因为可以通过<code>kube-router</code>获取路由信息直接路由到对应的节点上，就不需要再进行IP隧道/封装了</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 默认情况下的值</span><br>$ kubectl get cm -n kube-system cilium-config -o yaml | egrep <span class="hljs-string">&quot;tunnel|ipam&quot;</span><br>  ipam: cluster-pool<br>  tunnel: vxlan<br><span class="hljs-comment"># 修改前的路由条目</span><br>$ ip r<br>default via 10.31.254.253 dev eth0 proto static metric 100<br>10.31.0.0/16 dev eth0 proto kernel scope <span class="hljs-built_in">link</span> src 10.31.80.1 metric 100<br>10.32.0.0/24 via 10.32.3.145 dev cilium_host src 10.32.3.145 mtu 1450<br>10.32.1.0/24 via 10.32.3.145 dev cilium_host src 10.32.3.145 mtu 1450<br>10.32.2.0/24 via 10.32.3.145 dev cilium_host src 10.32.3.145 mtu 1450<br>10.32.3.0/24 via 10.32.3.145 dev cilium_host src 10.32.3.145<br>10.32.3.145 dev cilium_host scope <span class="hljs-built_in">link</span><br>10.32.4.0/24 via 10.32.3.145 dev cilium_host src 10.32.3.145 mtu 1450<br>10.32.5.0/24 via 10.32.3.145 dev cilium_host src 10.32.3.145 mtu 1450<br>  <br><span class="hljs-comment"># 修改后的值</span><br>$ kubectl get cm -n kube-system cilium-config -o yaml | egrep <span class="hljs-string">&quot;tunnel|ipam&quot;</span><br>  ipam: kubernetes<br>  tunnel: disabled<br>  <br><span class="hljs-comment"># 修改后的路由条目</span><br>$ ip r<br>default via 10.31.254.253 dev eth0 proto static metric 100<br>10.31.0.0/16 dev eth0 proto kernel scope <span class="hljs-built_in">link</span> src 10.31.80.1 metric 100<br>10.32.0.0/24 via 10.32.3.145 dev cilium_host src 10.32.3.145 mtu 1450<br>10.32.1.0/24 via 10.31.80.2 dev eth0 proto 17<br>10.32.2.0/24 via 10.31.80.3 dev eth0 proto 17<br>10.32.3.0/24 via 10.31.80.4 dev eth0 proto 17<br>10.32.3.145 dev cilium_host scope <span class="hljs-built_in">link</span><br>10.32.4.0/24 via 10.31.80.5 dev eth0 proto 17<br>10.32.5.0/24 via 10.31.80.6 dev eth0 proto 17<br></code></pre></div></td></tr></table></figure>
<blockquote>
<p>需要注意的是如果cilium-config配置里面没有<code>ipv4-native-routing-cidr</code>这个参数的话也需要加上，配置为pod的CIDR即可（<code>ipv4-native-routing-cidr: 10.32.0.0/17</code>），否则在重启cilium的时候会出现下面这个报错，关于Native-Routing的配置和原理可以参考官方的这个<a target="_blank" rel="noopener" href="https://docs.cilium.io/en/stable/concepts/networking/routing/#native-routing">文档</a></p>
<p>level=fatal msg=“Error while creating daemon” error=“invalid daemon configuration: native routing cidr must be configured with option --ipv4-native-routing-cidr in combination with --enable-ipv4-masquerade --tunnel=disabled --ipam=kubernetes --enable-ipv4=true” subsys=daemon</p>
</blockquote>
<p>可以明显的看到修改之后的路由条目默认情况下都不再通过封装的隧道接口<code>cilium_host</code>，而是直接通过kube-router发布的BGP路由直达对应的node节点上面。</p>
<h1>7、部署loadbalancer</h1>
<p>因为这里我们没有使用cilium的<code>withoutkubeproxy</code>模式，因此有部分cilium的高级功能无法使用，尽管我们前面已经把clusterIP和podIP都通过BGP宣告出去，在部分场景下面还是需要有LoadBalancer类型的服务作为补充。</p>
<p>目前市面上开源的K8S-LoadBalancer主要就是<a href="https://tinychen.com/20220519-k8s-06-loadbalancer-metallb/">MetalLB</a>、<a href="https://tinychen.com/20220523-k8s-07-loadbalancer-openelb/">OpenELB</a>和<a href="https://tinychen.com/20220524-k8s-08-loadbalancer-purelb/">PureLB</a>这三种，三者的工作原理和使用教程我都写文章分析过，针对目前这种使用场景，我个人认为最合适的是使用PureLB，因为他的组件高度模块化，并且可以自由选择实现ECMP模式的路由协议和软件（MetalLB和OpenELB都是自己通过gobgp实现的BGP协议），能更好的和我们前面的<code>cilium+kube-router</code>组合在一起。</p>
<h2 id="7-1-原理架构">7.1 原理架构</h2>
<p>关于purelb的详细工作原理和部署使用方式可以参考我之前写的<a href="https://tinychen.com/20220524-k8s-08-loadbalancer-purelb/">这篇文章</a>，这里不再赘述。</p>
<p><img src="https://resource.tinychen.com/202205241423207.png" srcset="/img/loading.gif" lazyload alt=""></p>
<ul>
<li><strong>Allocator</strong>：用来监听API中的<code>LoadBalancer</code>类型服务，并且负责分配IP。</li>
<li><strong>LBnodeagent</strong>： 作为<code>daemonset</code>部署到每个可以暴露请求并吸引流量的节点上，并且负责监听服务的状态变化同时负责把VIP添加到本地网卡或者是虚拟网卡</li>
<li><strong>KubeProxy</strong>：k8s的内置组件，并非是PureLB的一部分，但是PureLB依赖其进行正常工作，当对VIP的请求达到某个具体的节点之后，需要由kube-proxy来负责将其转发到对应的pod</li>
</ul>
<p>因为我们此前已经部署了kube-router，并且会由它来负责BGP宣告的相关操作，因此在这里我们直接使用purelb的BGP模式，并且不需要自己再额外部署bird或frr来进行BGP路由发布，同时也不需要<code>LBnodeagent</code>组件来帮助暴露并吸引流量，只需要<code>Allocator</code>帮助我们完成LoadBalancerIP的分配操作即可。</p>
<h2 id="7-2-部署purelb">7.2 部署purelb</h2>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 下载官方提供的yaml文件到本地进行部署</span><br>$ wget https://gitlab.com/api/v4/projects/purelb%2Fpurelb/packages/generic/manifest/0.0.1/purelb-complete.yaml<br><br><span class="hljs-comment"># 请注意，由于 Kubernetes 的最终一致性架构，此manifest清单的第一个应用程序可能会失败。发生这种情况是因为清单既定义了CRD，又使用该CRD创建了资源。如果发生这种情况，请再次应用manifest清单，应该就会部署成功。</span><br><br>$ kubectl apply -f purelb-complete.yaml<br>$ kubectl apply -f purelb-complete.yaml<br><br><br><br><br><span class="hljs-comment"># 检测部署后的各个资源及工作负载是否正常</span><br>$ kubectl get pods -n purelb -o wide<br>NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE                                       NOMINATED NODE   READINESS GATES<br>allocator-8657d47b5c-rdd46   1/1     Running   0          71s   10.32.3.149   k8s-cilium-worker-10-31-80-4.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>lbnodeagent-hnffq            1/1     Running   0          71s   10.31.80.4    k8s-cilium-worker-10-31-80-4.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>lbnodeagent-jzs8s            1/1     Running   0          71s   10.31.80.6    k8s-cilium-worker-10-31-80-6.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>lbnodeagent-qr4cb            1/1     Running   0          71s   10.31.80.5    k8s-cilium-worker-10-31-80-5.tinychen.io   &lt;none&gt;           &lt;none&gt;<br>$ kubectl get deploy -n purelb<br>NAME        READY   UP-TO-DATE   AVAILABLE   AGE<br>allocator   1/1     1            1           86s<br>$ kubectl get crd | grep purelb<br>lbnodeagents.purelb.io                       2022-12-09T12:38:13Z<br>servicegroups.purelb.io                      2022-12-09T12:38:13Z<br>$ kubectl get --namespace=purelb servicegroups.purelb.io<br>No resources found <span class="hljs-keyword">in</span> purelb namespace.<br>$ kubectl get --namespace=purelb lbnodeagent.purelb.io<br>NAME      AGE<br>default   89s<br><br><br><span class="hljs-comment"># 查看创建的相关API资源，注意这里的lbnodeagents我们此处用不到，可以忽略</span><br>$ kubectl api-resources --api-group=purelb.io<br>NAME            SHORTNAMES   APIVERSION     NAMESPACED   KIND<br>lbnodeagents    lbna,lbnas   purelb.io/v1   <span class="hljs-literal">true</span>         LBNodeAgent<br>servicegroups   sg,sgs       purelb.io/v1   <span class="hljs-literal">true</span>         ServiceGroup<br><br></code></pre></div></td></tr></table></figure>
<p><code>lbnodeagent</code>的这个ds我们这里用不到，因此可以直接删除。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl delete ds -n purelb lbnodeagent<br>daemonset.apps <span class="hljs-string">&quot;lbnodeagent&quot;</span> deleted<br>$ kubectl get ds -n purelb<br>No resources found <span class="hljs-keyword">in</span> purelb namespace.<br></code></pre></div></td></tr></table></figure>
<h2 id="7-3-配置IP池">7.3 配置IP池</h2>
<p>接下来我们部署一个ipam的sg，命名为<code>bgp-ippool</code>，ip段就使用我们预留的<code>10.32.192.0/18</code></p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ <span class="hljs-built_in">cat</span> purelb-ipam.yaml<br>apiVersion: purelb.io/v1<br>kind: ServiceGroup<br>metadata:<br>  name: bgp-ippool<br>  namespace: purelb<br>spec:<br>  <span class="hljs-built_in">local</span>:<br>    v4pool:<br>      subnet: <span class="hljs-string">&#x27;10.32.192.0/18&#x27;</span><br>      pool: <span class="hljs-string">&#x27;10.32.192.0-10.32.255.254&#x27;</span><br>      aggregation: /32<br>      <br>      <br>$ kubectl apply -f purelb-ipam.yaml<br>servicegroup.purelb.io/bgp-ippool created<br>$ kubectl get sg -n purelb<br>NAME         AGE<br>bgp-ippool   8s<br></code></pre></div></td></tr></table></figure>
<p>到这里我们的PureLB就部署完了，相比完整的ECMP模式要<strong>少部署了路由协议软件</strong>和<strong>额外删除了<code>lbnodeagent</code></strong>，接下来可以开始测试了。</p>
<h1>8、部署测试用例</h1>
<p>集群部署完成之后我们在k8s集群中部署一个nginx测试一下是否能够正常工作。首先我们创建一个名为<code>nginx-quic</code>的命名空间（<code>namespace</code>），然后在这个命名空间内创建一个名为<code>nginx-quic-deployment</code>的<code>deployment</code>用来部署pod，最后再创建一个<code>service</code>用来暴露服务，这里我们同时使用<code>nodeport</code>和<code>LoadBalancer</code>两种方式来暴露服务，并且其中一个<code>LoadBalancer</code>的服务还要指定<code>LoadBalancerIP</code>方便我们测试。</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-string">$</span> <span class="hljs-string">cat</span> <span class="hljs-string">nginx-quic.yaml</span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-quic</span><br><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-quic-deployment</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-quic</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">4</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-quic</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-quic</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">tinychen777/nginx-quic:latest</span><br>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span><br>        <span class="hljs-attr">ports:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><br><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-quic-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-quic</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>    <span class="hljs-attr">nodePort:</span> <span class="hljs-number">30088</span> <span class="hljs-comment"># match for external access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span><br><br><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">bgp-ippool</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">allocateLoadBalancerNodePorts:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">internalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-quic</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span><br>  <span class="hljs-attr">loadBalancerIP:</span> <span class="hljs-number">10.32</span><span class="hljs-number">.192</span><span class="hljs-number">.192</span><br><br><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">bgp-ippool</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb2-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">allocateLoadBalancerNodePorts:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">internalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-quic</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span><br></code></pre></div></td></tr></table></figure>
<p>部署完成后我们直接查看状态</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 直接部署</span><br>$ kubectl apply -f nginx-quic.yaml<br>namespace/nginx-quic created<br>deployment.apps/nginx-quic-deployment created<br>service/nginx-quic-service created<br><br><span class="hljs-comment"># 查看deployment的运行状态</span><br>$ kubectl get deployment -o wide -n nginx-quic<br>NAME                    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                          SELECTOR<br>nginx-quic-deployment   4/4     4            4           63m   nginx-quic   tinychen777/nginx-quic:latest   app=nginx-quic<br><br><span class="hljs-comment"># 查看service的运行状态</span><br>$ kubectl get svc -n nginx-quic<br>NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE<br>nginx-lb-service     LoadBalancer   10.32.169.6     10.32.192.192   80/TCP           86m<br>nginx-lb2-service    LoadBalancer   10.32.172.208   10.32.192.0     80/TCP           8m52s<br>nginx-quic-service   NodePort       10.32.176.164   &lt;none&gt;          8080:30088/TCP   3h9m<br><br><span class="hljs-comment"># 查看pod的运行状态</span><br>$ kubectl get pods -n nginx-quic<br>NAME                                     READY   STATUS    RESTARTS   AGE<br>nginx-quic-deployment-748867774b-75xrq   1/1     Running   0          3h14m<br>nginx-quic-deployment-748867774b-pwqpg   1/1     Running   0          3h14m<br>nginx-quic-deployment-748867774b-tm2p5   1/1     Running   0          3h14m<br>nginx-quic-deployment-748867774b-tw86v   1/1     Running   0          3h14m<br><br><span class="hljs-comment"># 查看IPVS规则</span><br>$ ipvsadm -lnt 10.32.176.164:8080<br>Prot LocalAddress:Port Scheduler Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  10.32.176.164:8080 rr<br>  -&gt; 10.32.3.138:80               Masq    1      0          0<br>  -&gt; 10.32.3.223:80               Masq    1      0          0<br>  -&gt; 10.32.4.80:80                Masq    1      0          0<br>  -&gt; 10.32.5.88:80                Masq    1      0          0<br><br>$ ipvsadm -lnt 10.31.80.1:30088<br>Prot LocalAddress:Port Scheduler Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  10.31.80.1:30088 rr<br>  -&gt; 10.32.3.138:80               Masq    1      0          0<br>  -&gt; 10.32.3.223:80               Masq    1      0          0<br>  -&gt; 10.32.4.80:80                Masq    1      0          0<br>  -&gt; 10.32.5.88:80                Masq    1      0          0<br>  <br>$ ipvsadm -lnt 10.32.192.192:80<br>Prot LocalAddress:Port Scheduler Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  10.32.192.192:80 rr<br>  -&gt; 10.32.3.138:80               Masq    1      0          0<br>  -&gt; 10.32.3.223:80               Masq    1      0          0<br>  -&gt; 10.32.4.80:80                Masq    1      0          0<br>  -&gt; 10.32.5.88:80                Masq    1      0          0<br>  <br>$ ipvsadm -lnt 10.32.192.0:80<br>Prot LocalAddress:Port Scheduler Flags<br>  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn<br>TCP  10.32.192.0:80 rr<br>  -&gt; 10.32.3.138:80               Masq    1      0          0<br>  -&gt; 10.32.3.223:80               Masq    1      0          0<br>  -&gt; 10.32.4.80:80                Masq    1      0          0<br>  -&gt; 10.32.5.88:80                Masq    1      0          0<br></code></pre></div></td></tr></table></figure>
<p>最后我们进行测试，这个nginx-quic的镜像默认情况下会返回在nginx容器中获得的用户请求的IP和端口，由于我们前面配置的时候把podIP和clusterIP都通过BGP发布出去了，因此我们在集群外直接访问podIP、clusterIP、nodeport和loadbalancerIP都可以成功访问。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># root @ tiny-openwrt in ~ [22:17:46]</span><br>$  curl 10.32.192.0<br>10.31.80.4:50151<br><br><span class="hljs-comment"># root @ tiny-openwrt in ~ [22:17:47]</span><br>$  curl 10.32.192.192<br>10.31.80.3:1969<br><br><span class="hljs-comment"># root @ tiny-openwrt in ~ [22:17:50]</span><br>$ curl 10.32.5.88:80<br>10.31.254.253:52160<br><br><span class="hljs-comment"># root @ tiny-openwrt in ~ [22:17:58]</span><br>$ curl 10.32.176.164:8080<br>10.31.80.5:20972<br><br><span class="hljs-comment"># root @ tiny-openwrt in ~ [22:18:02]</span><br>$ curl 10.31.80.4:30088<br>10.31.80.4:3220<br></code></pre></div></td></tr></table></figure>
<p>最后我们检查一下路由器侧的情况，正常情况下可以看到kube-router发布的ECMP路由：</p>
<figure class="highlight dns"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs dns">B&gt;* <span class="hljs-number">10.32.192.0</span>/<span class="hljs-number">32</span> [<span class="hljs-number">20</span>/<span class="hljs-number">0</span>] via <span class="hljs-number">10.31.80.1</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">14</span><br>  *                       via <span class="hljs-number">10.31.80.2</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">14</span><br>  *                       via <span class="hljs-number">10.31.80.3</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">14</span><br>  *                       via <span class="hljs-number">10.31.80.4</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">14</span><br>  *                       via <span class="hljs-number">10.31.80.5</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">14</span><br>  *                       via <span class="hljs-number">10.31.80.6</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">00</span>:<span class="hljs-number">00</span>:<span class="hljs-number">14</span><br>B&gt;* <span class="hljs-number">10.32.192.192</span>/<span class="hljs-number">32</span> [<span class="hljs-number">20</span>/<span class="hljs-number">0</span>] via <span class="hljs-number">10.31.80.1</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">01</span>:<span class="hljs-number">17</span>:<span class="hljs-number">54</span><br>  *                         via <span class="hljs-number">10.31.80.2</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">01</span>:<span class="hljs-number">17</span>:<span class="hljs-number">54</span><br>  *                         via <span class="hljs-number">10.31.80.3</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">01</span>:<span class="hljs-number">17</span>:<span class="hljs-number">54</span><br>  *                         via <span class="hljs-number">10.31.80.4</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">01</span>:<span class="hljs-number">17</span>:<span class="hljs-number">54</span><br>  *                         via <span class="hljs-number">10.31.80.5</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">01</span>:<span class="hljs-number">17</span>:<span class="hljs-number">54</span><br>  *                         via <span class="hljs-number">10.31.80.6</span>, eth0, weight <span class="hljs-number">1</span>, <span class="hljs-number">01</span>:<span class="hljs-number">17</span>:<span class="hljs-number">54</span><br></code></pre></div></td></tr></table></figure>
<p>到这里整个K8S集群就部署完成了。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/cloudnative/">cloudnative</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/centos/">centos</a>
                    
                      <a class="hover-with-bg" href="/tags/k8s/">k8s</a>
                    
                      <a class="hover-with-bg" href="/tags/docker/">docker</a>
                    
                      <a class="hover-with-bg" href="/tags/cilium/">cilium</a>
                    
                      <a class="hover-with-bg" href="/tags/purelb/">purelb</a>
                    
                      <a class="hover-with-bg" href="/tags/bgp/">bgp</a>
                    
                      <a class="hover-with-bg" href="/tags/containerd/">containerd</a>
                    
                      <a class="hover-with-bg" href="/tags/ebpf/">ebpf</a>
                    
                      <a class="hover-with-bg" href="/tags/xdp/">xdp</a>
                    
                      <a class="hover-with-bg" href="/tags/kube-router/">kube-router</a>
                    
                  </div>
                
              </div>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/20221222-k8s-11-kubernetes-without-kubeproxy/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">k8s系列11-cilium部署KubeProxyReplacement模式</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/20221120-dns-13-coredns-10-dnsredir-and-alternate/">
                        <span class="hidden-mobile">CoreDNS篇10-分流与重定向</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <i class="iconfont icon-copyright"></i> <a href="https://tinychen.com/" target="_blank" rel="nofollow noopener"><span>Since 2017 By TinyChen </span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Hexo-Fluid</span></a> 
  </div>
  

  
  <!-- 备案信息 -->
  <div class="beian">
    <span>
      <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
        粤ICP备18140640号
      </a>
    </span>
    
  </div>


  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>












  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?7a96963a1145ac7fde1442d739a11ffd";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  
    <!-- Google Analytics -->
    <script defer>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) };
      ga.l = +new Date;
      ga('create', 'UA-166769908-1', 'auto');
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
